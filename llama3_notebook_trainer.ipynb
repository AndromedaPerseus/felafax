{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install --upgrade transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['USE_TORCH'] = 'True'  # To use transformers library in TPU\n",
    "os.environ['XLA_USE_BF16'] = 'True'\n",
    "os.environ['PJRT_DEVICE'] = 'TPU'\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 13.608021,
     "end_time": "2023-11-04T12:34:21.013846",
     "exception": false,
     "start_time": "2023-11-04T12:34:07.405825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_25412/1142697285.py:14: DeprecationWarning: Importing from `torch_xla.experimental.xla_sharded_tensor` will be deprecated after 2.2 release. Please use `torch_xla.distributed.spmd` instead.\n",
      "  from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
      "/tmp/ipykernel_25412/1142697285.py:15: DeprecationWarning: Importing from `torch_xla.experimental.xla_sharding` will be deprecated after 2.2 release. Please use `torch_xla.distributed.spmd` instead.\n",
      "  from torch_xla.experimental.xla_sharding import Mesh\n",
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import contextlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.runtime as xr\n",
    "xr.use_spmd()\n",
    "xr.initialize_cache('/mnt/persistent-disk/xla/')\n",
    "\n",
    "import torch_xla.distributed.spmd as xs\n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
    "from torch_xla.experimental.xla_sharding import Mesh\n",
    "\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "from safetensors.torch import load_file\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xr.is_spmd()==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LLaMa3.model_partitioning' from '/home/RoadrunnerX/LLaMa3/model_partitioning.py'>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append('')\n",
    "model_partitioning = importlib.import_module('LLaMa3.model_partitioning')\n",
    "importlib.reload(model_partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model, num_layers=1):\n",
    "    print_is_xla()\n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "        if i >= num_layers:\n",
    "            break\n",
    "        \n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Device: {param.device}\")\n",
    "        print(f\"Tensor value: {param.data}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "def print_is_xla():\n",
    "    import torch_xla\n",
    "    print(\"is xla: \", torch_xla._XLAC._xla_runtime_is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On a single TPU VM host, you can train/tune LLaMa 3/3.1 8B models with full precision or LoRA.\n",
    "supported_models = [\n",
    "    \"TinyLlama/TinyLlama-1.1B-step-50K-105b\",\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    \"meta-llama/Meta-Llama-3-8B\",\n",
    "    \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "]\n",
    "\n",
    "# Select a supported model from above list to use!\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3.1-8B\"\n",
    "HUGGINGFACE_TOKEN = \"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\"\n",
    "DEBUG_MODE = False\n",
    "\n",
    "TRAINER_CONFIG = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 1,\n",
    "    \"max_length\": 512,\n",
    "    \n",
    "    \"lr\": 5e-5,\n",
    "    \"logging_interval\": 5,  # logs every 5 steps\n",
    "    \n",
    "    \"lora_rank\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure LoRA config for your model.\n",
    "Use the below code to configure the LoRA config for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(*, model, lora_rank=None, lora_alpha=None, lora_dropout=None):\n",
    "    \"\"\"Applies LoRA configuration to the model.\"\"\"\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8 if not lora_rank else lora_rank,\n",
    "        lora_alpha=32 if not lora_alpha else lora_alpha,\n",
    "        lora_dropout=0.1 if not lora_dropout else lora_dropout,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(*, model_name, hugging_face_token):\n",
    "    \"\"\"Downloads and initializes the model.\"\"\"\n",
    "    config = AutoConfig.from_pretrained(\n",
    "        model_name, \n",
    "        token=hugging_face_token)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        token=hugging_face_token\n",
    "    )\n",
    "\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name, \n",
    "        token=hugging_face_token,\n",
    "        low_cpu_mem_usage=True\n",
    "    )\n",
    "\n",
    "    model = apply_lora(\n",
    "        model=model,\n",
    "        lora_rank=TRAINER_CONFIG[\"lora_rank\"],\n",
    "        lora_alpha=TRAINER_CONFIG[\"lora_alpha\"],\n",
    "        lora_dropout=TRAINER_CONFIG[\"lora_dropout\"],\n",
    "    )\n",
    "\n",
    "    model = xmp.MpModelWrapper(model)\n",
    "\n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.547357,
     "end_time": "2023-11-04T12:34:26.034497",
     "exception": false,
     "start_time": "2023-11-04T12:34:25.48714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_spmd(*, model, mesh):\n",
    "    # Apply on layers within model.\n",
    "    model_partitioning_util.partition_model(model, mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure dataset pipeline for your model\n",
    "\n",
    "For this project, we're utilizing the refined **Alpaca dataset**, curated by yahma. This dataset is a carefully filtered selection of 52,000 entries from the original Alpaca collection. Feel free to substitute this section with your own data preparation code if you prefer.\n",
    "\n",
    "It's crucial to include the EOS_TOKEN (End of Sequence Token) in your tokenized output. Failing to do so may result in endless generation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size=None, max_length=None, debug_mode=False):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    # Tokenize the dataset.\n",
    "    def _tokenize(examples):\n",
    "        # Tokenized is list within list. Compute labels for causalLM by shifting input_id; \n",
    "        # consequently truncate input_id to penultimate position.\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512+1 if not max_length else max_length+1)\n",
    "        labels = tokenized['input_ids'].copy()\n",
    "        tokenized['labels'] = [label[1:] for label in labels]\n",
    "        tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "        return tokenized\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    if debug_mode:\n",
    "        dataset = dataset.select(range(32)) # Use just 32 exampfor faster iteration\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=1 if not batch_size else batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=1 if not batch_size else batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Now let's train the model. We are using PyTorch XLA's Fully Sharded Data Parallel (FSDP) to distribute the model across the 8 TPU cores available on TPU v3-8. This approach allows for efficient training on TPU hardware. We also utilize PyTorch/XLA's MpDeviceLoader to efficiently load data onto the TPU cores.\n",
    "\n",
    "**NOTE:** It's important to note that the **first step of training will be slow**. This is because XLA takes time initially to compile the computational graph. However, once the compilation is complete, subsequent steps will run much faster using compiled+cached graph, and leveraging the full power of the all TPU cores for accelerated training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_update(device,\n",
    "                          step,\n",
    "                          loss,\n",
    "                          rate,\n",
    "                          epoch=None):\n",
    "    \"\"\"Prints the training metrics at a given step.\"\"\"\n",
    "    if xm.is_master_ordinal():  # Only print on the master device\n",
    "        update_data = [\n",
    "            'Training',\n",
    "            f'Epoch={epoch}' if epoch is not None else None,\n",
    "            f'Step={step}',\n",
    "            f'Loss={loss:.5f}',\n",
    "            # f'Rate={rate:.2f}',\n",
    "            # f'Time={now()}'\n",
    "        ]\n",
    "        print(' | '.join(item for item in update_data if item), flush=True)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:02<00:00,  1.73it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = init_model(\n",
    "        model_name=MODEL_NAME, hugging_face_token=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    global model, tokenizer\n",
    "\n",
    "    torch.manual_seed(99)\n",
    "    device = xm.xla_device()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create a mesh for the model partitioning.\n",
    "    num_devices = xr.global_runtime_device_count()\n",
    "    mesh_shape = (1, num_devices, 1)\n",
    "    device_ids = np.array(range(num_devices))\n",
    "    mesh = Mesh(device_ids, mesh_shape, (\"dp\", \"fsdp\", \"mp\"))\n",
    "\n",
    "    # model = checkpoint_module(model)\n",
    "    \n",
    "    # Partition the model using SPMD.\n",
    "    model_partitioning.partition_model(model=model, mesh=mesh)\n",
    "    \n",
    "    # Configure the training loop.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=TRAINER_CONFIG[\"lr\"])\n",
    "\n",
    "    train_dataloader, test_dataloader = get_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=TRAINER_CONFIG[\"batch_size\"],\n",
    "        max_length=TRAINER_CONFIG[\"max_length\"],\n",
    "    )\n",
    "    train_dataloader = pl.MpDeviceLoader(\n",
    "        train_dataloader, \n",
    "        device\n",
    "    ) \n",
    "    test_dataloader = pl.MpDeviceLoader(\n",
    "        test_dataloader, \n",
    "        device\n",
    "    )\n",
    "\n",
    "    for epoch in range(TRAINER_CONFIG[\"epochs\"]):\n",
    "        xm.master_print(f\"Epoch {epoch} train begin {test_utils.now()}\")\n",
    "        tracker = xm.RateTracker()\n",
    "        \n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if step>10:\n",
    "                break\n",
    "                \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"labels\"],\n",
    "            )\n",
    "            xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "            xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "            xs.mark_sharding(labels, mesh, (0, 1))\n",
    "            \n",
    "            output = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            xm.mark_step()\n",
    "\n",
    "            if step%TRAINER_CONFIG[\"logging_interval\"]==0:\n",
    "                loss_cpu = loss.item()\n",
    "                xm.add_step_closure(\n",
    "                    print_training_update,\n",
    "                    args=(device, step, loss_cpu, tracker.rate(), epoch)\n",
    "                )\n",
    "            \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(test_dataloader):\n",
    "                input_ids, attention_mask, labels = (\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    batch[\"labels\"],\n",
    "                )\n",
    "                xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "                xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "                xs.mark_sharding(labels, mesh, (0, 1))\n",
    "                \n",
    "                output = model(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "                )\n",
    "                eval_loss += output.loss.item()\n",
    "        avg_eval_loss = eval_loss / len(test_dataloader)\n",
    "        xm.add_step_closure(\n",
    "            lambda: print(f\"Eval loss: {avg_eval_loss:.4f}\"),\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722464499.867909   25675 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722464499.867909   25674 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722464499.867989   25674 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722464499.867988   25675 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722464499.868004   25675 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "I0000 00:00:1722464499.868002   25674 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722464499.883581   25672 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722464499.883637   25672 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722464499.883647   25672 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722464499.883960   25673 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722464499.884028   25673 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722464499.884041   25673 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "Map: 100%|██████████| 43996/43996 [00:23<00:00, 1877.46 examples/s]\n",
      "Map: 100%|██████████| 7764/7764 [00:05<00:00, 1496.64 examples/s]s]\n",
      "Map:  66%|██████▌   | 29000/43996 [00:17<00:08, 1695.47 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 22:22:30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 43996/43996 [00:25<00:00, 1694.57 examples/s]\n",
      "Map:  75%|███████▌  | 33000/43996 [00:19<00:07, 1449.05 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 22:22:33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 43996/43996 [00:25<00:00, 1720.92 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 22:22:36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 43996/43996 [00:24<00:00, 1783.70 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 22:22:38\n",
      "Training | Epoch=0 | Step=0 | Loss=13.53303\n",
      "\n",
      "Training | Epoch=0 | Step=5 | Loss=12.80109\n",
      "\n",
      "Training | Epoch=0 | Step=10 | Loss=12.47537\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n",
      "We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'float' object has no attribute 'item'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 95, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 78, in _run_thread_per_device\n    replica_results = list(\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\n    yield _result_or_cancel(fs.pop())\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\n    return fut.result(timeout)\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n    return self.__get_result()\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 71, in _thread_fn\n    return fn()\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 187, in __call__\n    self.fn(runtime.global_ordinal(), *self.args, **self.kwargs)\n  File \"/tmp/ipykernel_25412/3242716415.py\", line 90, in train\n    avg_eval_loss = avg_eval_loss.item()\nAttributeError: 'float' object has no attribute 'item'\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:211\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    209\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 211\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:95\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     92\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     93\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 95\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:171\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    166\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    167\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    168\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    169\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    170\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[0;32m--> 171\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m      \u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m          \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess_results\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _merge_replica_results(replica_results)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:172\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    165\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    166\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    167\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    168\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    169\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    170\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[1;32m    171\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m--> 172\u001b[0m       itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m    173\u001b[0m           result\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m process_results))\n\u001b[1;32m    175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _merge_replica_results(replica_results)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'float' object has no attribute 'item'"
     ]
    }
   ],
   "source": [
    "xmp.spawn(train, args=(), start_method=\"fork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model to HuggingFace Hub\n",
    "Uncoment the following cell to push the model to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /mnt/persistent-disk/hf/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=HUGGINGFACE_TOKEN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "papermill": {
     "duration": 388.410448,
     "end_time": "2023-11-04T13:21:54.038795",
     "exception": false,
     "start_time": "2023-11-04T13:15:25.628347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'MpModelWrapper' object has no attribute 'cpu'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m()\n\u001b[1;32m      2\u001b[0m model\u001b[38;5;241m.\u001b[39mpush_to_hub(\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfelarof01/llama3-8.1-roadrunner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      4\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      7\u001b[0m     max_shard_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m5GB\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      8\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'MpModelWrapper' object has no attribute 'cpu'"
     ]
    }
   ],
   "source": [
    "model = model.cpu()\n",
    "model.push_to_hub(\n",
    "    \"felarof01/llama3-8.1-roadrunner\",\n",
    "    tokenizer=tokenizer,\n",
    "    private=False,\n",
    "    create_pr=False,\n",
    "    max_shard_size=\"5GB\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3555678,
     "isSourceIdPinned": true,
     "sourceId": 6196932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3936750,
     "sourceId": 6847931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946973,
     "sourceId": 6867914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937441,
     "sourceId": 6868189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949797,
     "sourceId": 6873567,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3944051,
     "sourceId": 7060310,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30529,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
