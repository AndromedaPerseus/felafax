{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade transformers -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export USE_TORCH=True # To use transformers library in TPU\n",
    "!export PJRT_DEVICE=TPU\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\"\n",
    "!export XLA_USE_BF16=True\n",
    "\n",
    "import os\n",
    "os.environ['USE_TORCH'] = 'True'  # To use transformers library in TPU\n",
    "os.environ['XLA_USE_BF16'] = 'True'\n",
    "os.environ['PJRT_DEVICE'] = 'TPU'\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 13.608021,
     "end_time": "2023-11-04T12:34:21.013846",
     "exception": false,
     "start_time": "2023-11-04T12:34:07.405825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:242: UserWarning: XLA_USE_SPMD is being deprecated. Use torch_xla.runtime.use_spmd() without setting XLA_USE_SPMD env-var.\n",
      "  warnings.warn(\"XLA_USE_SPMD is being deprecated. \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import contextlib\n",
    "from dataclasses import dataclass\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.distributed as dist\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.runtime as xr\n",
    "xr.use_spmd()\n",
    "\n",
    "import torch_xla.experimental.xla_sharding as xs \n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
    "from torch_xla.experimental.xla_sharding import Mesh\n",
    "\n",
    "# from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as FSDP, checkpoint_module\n",
    "\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, TaskType, get_peft_model\n",
    "\n",
    "import logging\n",
    "logging.getLogger(\"datasets\").setLevel(logging.WARNING)\n",
    "logging.getLogger(\"transformers\").setLevel(logging.WARNING)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert xr.is_spmd()==True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'LLaMa3.model_partitioning' from '/home/RoadrunnerX/LLaMa3/model_partitioning.py'>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append('')\n",
    "model_partitioning = importlib.import_module('LLaMa3.model_partitioning')\n",
    "importlib.reload(model_partitioning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /mnt/persistent-disk/hf/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = \"meta-llama/Meta-Llama-3-70B\"\n",
    "MODEL_NAME=model_path\n",
    "# model, tokenizer = load_tinyllama_low_mem_tpu(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n",
    "from safetensors.torch import load_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_model(model, num_layers=1):\n",
    "    print_is_xla()\n",
    "    for i, (name, param) in enumerate(model.named_parameters()):\n",
    "        if i >= num_layers:\n",
    "            break\n",
    "        \n",
    "        print(f\"Layer: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"Device: {param.device}\")\n",
    "        print(f\"Tensor value: {param.data}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "def print_is_xla():\n",
    "    import torch_xla\n",
    "    print(\"is xla: \", torch_xla._XLAC._xla_runtime_is_initialized())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a supported model from above list to use!\n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-70B\"\n",
    "HUGGINGFACE_TOKEN = \"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\"\n",
    "DEBUG_MODE = False\n",
    "\n",
    "TRAINER_CONFIG = {\n",
    "    \"epochs\": 1,\n",
    "    \"batch_size\": 1,\n",
    "    \"max_length\": 512,\n",
    "    \n",
    "    \"lr\": 5e-5,\n",
    "    \"logging_interval\": 5,  # logs every 5 steps\n",
    "    \n",
    "    \"lora_rank\": 8,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.1,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(*, model, lora_rank=None, lora_alpha=None, lora_dropout=None):\n",
    "    \"\"\"Applies LoRA configuration to the model.\"\"\"\n",
    "    peft_config = LoraConfig(\n",
    "        task_type=TaskType.CAUSAL_LM,\n",
    "        inference_mode=False,\n",
    "        r=8 if not lora_rank else lora_rank,\n",
    "        lora_alpha=32 if not lora_alpha else lora_alpha,\n",
    "        lora_dropout=0.1 if not lora_dropout else lora_dropout,\n",
    "    )\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(*, model_name, hugging_face_token):\n",
    "    \"\"\"Downloads and initializes the model.\"\"\"\n",
    "    from huggingface_hub import hf_hub_download\n",
    "    \n",
    "    config = AutoConfig.from_pretrained(model_path)\n",
    "    with torch.device('meta'):\n",
    "        model = AutoModelForCausalLM.from_config(config)\n",
    "\n",
    "    # state_dict_path = hf_hub_download(repo_id=model_path, filename=\"pytorch_model.bin\")\n",
    "    # state_dict = torch.load(state_dict_path, map_location=\"cpu\")\n",
    "    \n",
    "    from safetensors.torch import load_file\n",
    "    # Get the list of safetensor files\n",
    "    files = [\n",
    "        \"model-00001-of-00030.safetensors\",\n",
    "        \"model-00002-of-00030.safetensors\",\n",
    "        \"model-00003-of-00030.safetensors\",\n",
    "        \"model-00004-of-00030.safetensors\",\n",
    "        \"model-00005-of-00030.safetensors\",\n",
    "        \"model-00006-of-00030.safetensors\",\n",
    "        \"model-00007-of-00030.safetensors\",\n",
    "        \"model-00008-of-00030.safetensors\",\n",
    "        \"model-00009-of-00030.safetensors\",\n",
    "        \"model-00010-of-00030.safetensors\",\n",
    "        \"model-00011-of-00030.safetensors\",\n",
    "        \"model-00012-of-00030.safetensors\",\n",
    "        \"model-00013-of-00030.safetensors\",\n",
    "        \"model-00014-of-00030.safetensors\",\n",
    "        \"model-00015-of-00030.safetensors\",\n",
    "        \"model-00016-of-00030.safetensors\",\n",
    "        \"model-00017-of-00030.safetensors\",\n",
    "        \"model-00018-of-00030.safetensors\",\n",
    "        \"model-00019-of-00030.safetensors\",\n",
    "        \"model-00020-of-00030.safetensors\",\n",
    "        \"model-00021-of-00030.safetensors\",\n",
    "        \"model-00022-of-00030.safetensors\",\n",
    "        \"model-00023-of-00030.safetensors\",\n",
    "        \"model-00024-of-00030.safetensors\",\n",
    "        \"model-00025-of-00030.safetensors\",\n",
    "        \"model-00026-of-00030.safetensors\",\n",
    "        \"model-00027-of-00030.safetensors\",\n",
    "        \"model-00028-of-00030.safetensors\",\n",
    "        \"model-00029-of-00030.safetensors\",\n",
    "        \"model-00030-of-00030.safetensors\"\n",
    "    ]\n",
    "    \n",
    "    # Load and merge state dicts\n",
    "    state_dict = {}\n",
    "    for file in files:\n",
    "        print(f\"downloading file {file} from HF...\")\n",
    "        file_path = hf_hub_download(repo_id=model_path, filename=file)\n",
    "        state_dict.update(load_file(file_path))\n",
    "\n",
    "    for name, param in model.named_parameters():\n",
    "        if name in state_dict:\n",
    "            # empty_param = torch.empty_like(state_dict[name], dtype=state_dict[name].dtype)\n",
    "            # if param.shape != empty_param.shape:\n",
    "            #     raise ValueError(f\"Shape mismatch for {name}: param {param.shape} vs state_dict {empty_param.shape}\")\n",
    "\n",
    "            # param.data = empty_param.to(param.device)\n",
    "            param.data.copy_(state_dict[name].data)\n",
    "\n",
    "    model.to_empty(device='cpu')\n",
    "    print_model(model)\n",
    "    \n",
    "    model.load_state_dict(state_dict, strict=False, assign=True)\n",
    "    print_model(model)\n",
    "\n",
    "    model = apply_lora(\n",
    "        model=model,\n",
    "        lora_rank=TRAINER_CONFIG[\"lora_rank\"],\n",
    "        lora_alpha=TRAINER_CONFIG[\"lora_alpha\"],\n",
    "        lora_dropout=TRAINER_CONFIG[\"lora_dropout\"],\n",
    "    )\n",
    "\n",
    "    model = xmp.MpModelWrapper(model)\n",
    "    \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name, \n",
    "        token=hugging_face_token\n",
    "    )\n",
    "\n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        config.pad_token_id = tokenizer.pad_token_id\n",
    "        \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "papermill": {
     "duration": 0.547357,
     "end_time": "2023-11-04T12:34:26.034497",
     "exception": false,
     "start_time": "2023-11-04T12:34:25.48714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_spmd(*, model, mesh):\n",
    "    # Apply on layers within model.\n",
    "    model_partitioning_util.partition_model(model, mesh)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure dataset pipeline for your model\n",
    "\n",
    "For this project, we're utilizing the refined **Alpaca dataset**, curated by yahma. This dataset is a carefully filtered selection of 52,000 entries from the original Alpaca collection. Feel free to substitute this section with your own data preparation code if you prefer.\n",
    "\n",
    "It's crucial to include the EOS_TOKEN (End of Sequence Token) in your tokenized output. Failing to do so may result in endless generation loops."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size=None, max_length=None, debug_mode=False):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    # Tokenize the dataset.\n",
    "    def _tokenize(examples):\n",
    "        # Tokenized is list within list. Compute labels for causalLM by shifting input_id; \n",
    "        # consequently truncate input_id to penultimate position.\n",
    "        tokenized = tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512+1 if not max_length else max_length+1)\n",
    "        labels = tokenized['input_ids'].copy()\n",
    "        tokenized['labels'] = [label[1:] for label in labels]\n",
    "        tokenized['input_ids'] = [input_id[:-1] for input_id in tokenized['input_ids']]\n",
    "        return tokenized\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    if debug_mode:\n",
    "        dataset = dataset.select(range(32)) # Use just 32 exampfor faster iteration\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=1 if not batch_size else batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=1 if not batch_size else batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model\n",
    "\n",
    "Now let's train the model. We are using PyTorch XLA's Fully Sharded Data Parallel (FSDP) to distribute the model across the 8 TPU cores available on TPU v3-8. This approach allows for efficient training on TPU hardware. We also utilize PyTorch/XLA's MpDeviceLoader to efficiently load data onto the TPU cores.\n",
    "\n",
    "**NOTE:** It's important to note that the **first step of training will be slow**. This is because XLA takes time initially to compile the computational graph. However, once the compilation is complete, subsequent steps will run much faster using compiled+cached graph, and leveraging the full power of the all TPU cores for accelerated training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_training_update(device,\n",
    "                          step,\n",
    "                          loss,\n",
    "                          rate,\n",
    "                          epoch=None):\n",
    "    \"\"\"Prints the training metrics at a given step.\"\"\"\n",
    "    if xm.is_master_ordinal():  # Only print on the master device\n",
    "        update_data = [\n",
    "            'Training',\n",
    "            f'Epoch={epoch}' if epoch is not None else None,\n",
    "            f'Step={step}',\n",
    "            f'Loss={loss:.5f}',\n",
    "            # f'Rate={rate:.2f}',\n",
    "            # f'Time={now()}'\n",
    "        ]\n",
    "        print(' | '.join(item for item in update_data if item), flush=True)\n",
    "        print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading file model-00001-of-00030.safetensors from HF...\n",
      "downloading file model-00002-of-00030.safetensors from HF...\n",
      "downloading file model-00003-of-00030.safetensors from HF...\n",
      "downloading file model-00004-of-00030.safetensors from HF...\n",
      "downloading file model-00005-of-00030.safetensors from HF...\n",
      "downloading file model-00006-of-00030.safetensors from HF...\n",
      "downloading file model-00007-of-00030.safetensors from HF...\n",
      "downloading file model-00008-of-00030.safetensors from HF...\n",
      "downloading file model-00009-of-00030.safetensors from HF...\n",
      "downloading file model-00010-of-00030.safetensors from HF...\n",
      "downloading file model-00011-of-00030.safetensors from HF...\n",
      "downloading file model-00012-of-00030.safetensors from HF...\n",
      "downloading file model-00013-of-00030.safetensors from HF...\n",
      "downloading file model-00014-of-00030.safetensors from HF...\n",
      "downloading file model-00015-of-00030.safetensors from HF...\n",
      "downloading file model-00016-of-00030.safetensors from HF...\n",
      "downloading file model-00017-of-00030.safetensors from HF...\n",
      "downloading file model-00018-of-00030.safetensors from HF...\n",
      "downloading file model-00019-of-00030.safetensors from HF...\n",
      "downloading file model-00020-of-00030.safetensors from HF...\n",
      "downloading file model-00021-of-00030.safetensors from HF...\n",
      "downloading file model-00022-of-00030.safetensors from HF...\n",
      "downloading file model-00023-of-00030.safetensors from HF...\n",
      "downloading file model-00024-of-00030.safetensors from HF...\n",
      "downloading file model-00025-of-00030.safetensors from HF...\n",
      "downloading file model-00026-of-00030.safetensors from HF...\n",
      "downloading file model-00027-of-00030.safetensors from HF...\n",
      "downloading file model-00028-of-00030.safetensors from HF...\n",
      "downloading file model-00029-of-00030.safetensors from HF...\n",
      "downloading file model-00030-of-00030.safetensors from HF...\n",
      "is xla:  False\n",
      "Layer: model.embed_tokens.weight\n",
      "Shape: torch.Size([128256, 8192])\n",
      "Device: cpu\n",
      "Tensor value: tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "--------------------------------------------------\n",
      "is xla:  False\n",
      "Layer: model.embed_tokens.weight\n",
      "Shape: torch.Size([128256, 8192])\n",
      "Device: cpu\n",
      "Tensor value: tensor([[ 9.4604e-03, -5.7678e-03,  1.1673e-03,  ..., -3.6621e-03,\n",
      "         -2.2583e-03, -8.2397e-04],\n",
      "        [ 5.4932e-03, -6.3705e-04,  2.3651e-03,  ...,  6.7444e-03,\n",
      "          1.2064e-04,  6.0730e-03],\n",
      "        [ 6.8359e-03,  4.4861e-03, -3.2043e-03,  ...,  9.4604e-04,\n",
      "          1.2360e-03,  3.4485e-03],\n",
      "        ...,\n",
      "        [ 3.2131e-08,  1.7579e-08, -2.2259e-07,  ..., -3.7625e-07,\n",
      "          7.0315e-08, -2.9616e-07],\n",
      "        [ 7.2643e-08, -8.9407e-08,  4.3213e-07,  ..., -2.9802e-07,\n",
      "          4.5262e-07, -1.9744e-07],\n",
      "        [ 2.1979e-07,  3.4645e-07, -2.6636e-07,  ...,  3.1479e-07,\n",
      "          4.6194e-07, -2.3749e-07]], dtype=torch.bfloat16)\n",
      "--------------------------------------------------\n",
      "trainable params: 16,384,000 || all params: 70,570,090,496 || trainable%: 0.023216634532909754\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer = init_model(\n",
    "        model_name=MODEL_NAME, hugging_face_token=HUGGINGFACE_TOKEN\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    global model, tokenizer\n",
    "    torch.manual_seed(99)\n",
    "    device = xm.xla_device()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Create a mesh for the model partitioning.\n",
    "    num_devices = xr.global_runtime_device_count()\n",
    "    mesh_shape = (1, num_devices, 1)\n",
    "    device_ids = np.array(range(num_devices))\n",
    "    mesh = Mesh(device_ids, mesh_shape, (\"dp\", \"fsdp\", \"mp\"))\n",
    "    \n",
    "    # # model = checkpoint_module(model)\n",
    "\n",
    "    # Partition the model using SPMD.\n",
    "    model_partitioning.partition_model(model=model, mesh=mesh)\n",
    "    \n",
    "    # Configure the training loop.\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=TRAINER_CONFIG[\"lr\"])\n",
    "\n",
    "    train_dataloader, test_dataloader = get_dataset(\n",
    "        tokenizer=tokenizer,\n",
    "        batch_size=TRAINER_CONFIG[\"batch_size\"],\n",
    "        max_length=TRAINER_CONFIG[\"max_length\"],\n",
    "    )\n",
    "    train_dataloader = pl.MpDeviceLoader(\n",
    "        train_dataloader, \n",
    "        device\n",
    "    ) \n",
    "    test_dataloader = pl.MpDeviceLoader(\n",
    "        test_dataloader, \n",
    "        device\n",
    "    )\n",
    "\n",
    "    for epoch in range(TRAINER_CONFIG[\"epochs\"]):\n",
    "        xm.master_print(f\"Epoch {epoch} train begin {test_utils.now()}\")\n",
    "        tracker = xm.RateTracker()\n",
    "        \n",
    "        model.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            input_ids, attention_mask, labels = (\n",
    "                batch[\"input_ids\"],\n",
    "                batch[\"attention_mask\"],\n",
    "                batch[\"labels\"],\n",
    "            )\n",
    "            xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "            xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "            xs.mark_sharding(labels, mesh, (0, 1))\n",
    "            \n",
    "            output = model(\n",
    "                input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "            )\n",
    "            loss = output.loss\n",
    "            loss.backward()\n",
    "            \n",
    "            optimizer.step()\n",
    "            xm.mark_step()\n",
    "\n",
    "            if step%TRAINER_CONFIG[\"logging_interval\"]==0:\n",
    "                loss_cpu = loss.item()\n",
    "                xm.add_step_closure(\n",
    "                    print_training_update,\n",
    "                    args=(device, step, loss_cpu, tracker.rate(), epoch)\n",
    "                )\n",
    "            \n",
    "        model.eval()\n",
    "        eval_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(test_dataloader):\n",
    "                input_ids, attention_mask, labels = (\n",
    "                    batch[\"input_ids\"],\n",
    "                    batch[\"attention_mask\"],\n",
    "                    batch[\"labels\"],\n",
    "                )\n",
    "                xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "                xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "                xs.mark_sharding(labels, mesh, (0, 1))\n",
    "                \n",
    "                output = model(\n",
    "                    input_ids=input_ids, attention_mask=attention_mask, labels=labels\n",
    "                )\n",
    "                eval_loss += output.loss.item()\n",
    "        avg_eval_loss = eval_loss / len(test_dataloader)\n",
    "        xm.master_print(f'Epoch {epoch} eval loss: {avg_eval_loss:.2f}')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722269364.126077   19115 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722269364.126159   19115 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722269364.126171   19115 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722269364.137925   19116 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722269364.137907   19117 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722269364.137988   19116 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722269364.137988   19117 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722269364.137999   19116 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "I0000 00:00:1722269364.138001   19117 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1722269364.141785   19118 pjrt_api.cc:100] GetPjrtApi was found for tpu at /usr/local/lib/python3.10/site-packages/libtpu/libtpu.so\n",
      "I0000 00:00:1722269364.141845   19118 pjrt_api.cc:79] PJRT_Api is set for device type tpu\n",
      "I0000 00:00:1722269364.141856   19118 pjrt_api.cc:146] The PJRT plugin has PJRT API version 0.46. The framework PJRT API version is 0.46.\n",
      "Map: 100%|██████████| 43996/43996 [00:19<00:00, 2219.32 examples/s]\n",
      "Map: 100%|██████████| 7764/7764 [00:03<00:00, 2575.24 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 16:10:40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7764/7764 [00:03<00:00, 2458.18 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 16:10:44\n",
      "Epoch 0 train begin 16:10:49\n",
      "Epoch 0 train begin 16:11:01\n",
      "Training | Epoch=0 | Step=0 | Loss=14.32894\n",
      "\n"
     ]
    }
   ],
   "source": [
    "xmp.spawn(train, args=(), start_method=\"fork\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export the model to HuggingFace Hub\n",
    "Uncoment the following cell to push the model to HuggingFace Hub."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 388.410448,
     "end_time": "2023-11-04T13:21:54.038795",
     "exception": false,
     "start_time": "2023-11-04T13:15:25.628347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = model.cpu()\n",
    "# model.push_to_hub(\n",
    "#     \"<USERNAME>/llama3-finetuned\",\n",
    "#     tokenizer=tokenizer,\n",
    "#     private=False,\n",
    "#     create_pr=False,\n",
    "#     max_shard_size=\"2GB\",\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3555678,
     "isSourceIdPinned": true,
     "sourceId": 6196932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3936750,
     "sourceId": 6847931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946973,
     "sourceId": 6867914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937441,
     "sourceId": 6868189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949797,
     "sourceId": 6873567,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3944051,
     "sourceId": 7060310,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30529,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
