{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 121.512757,
     "end_time": "2023-11-04T12:34:07.401258",
     "exception": false,
     "start_time": "2023-11-04T12:32:05.888501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip -q\n",
    "# !pip install transformers datasets sentencepiece peft -q\n",
    "# !pip install huggingface_hub -q\n",
    "# !pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export USE_TORCH=True # To use transformers library in TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 13.608021,
     "end_time": "2023-11-04T12:34:21.013846",
     "exception": false,
     "start_time": "2023-11-04T12:34:07.405825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch_xla.core.xla_model as xm\n",
    "from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as FSDP, checkpoint_module\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.fsdp' from '/home/tunerX/utils/fsdp.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append('')\n",
    "fsdp_util = importlib.import_module('utils.fsdp')\n",
    "importlib.reload(fsdp_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "# \"meta-llama/Meta-Llama-3-8B\" \n",
    "# \"meta-llama/Llama-2-7b-hf\" \n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(*, model_name):\n",
    "    config = AutoConfig.from_pretrained(model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model):\n",
    "    # TODO: pass lora config as argument to function.\n",
    "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                             inference_mode=False,\n",
    "                             r=8,\n",
    "                             lora_alpha=32,\n",
    "                             lora_dropout=0.1)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsdp_wrapper(x):\n",
    "    return FSDP(x, shard_param_on_dim_0=True, pin_layout_in_collective_ops=True, disable_reshard_on_root=False, reshard_after_forward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "papermill": {
     "duration": 0.547357,
     "end_time": "2023-11-04T12:34:26.034497",
     "exception": false,
     "start_time": "2023-11-04T12:34:25.48714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_fsdp(model):\n",
    "    # Apply on layers within model.\n",
    "    fsdp_util.apply_fsdp(model, [\"LlamaDecoderLayer\"])\n",
    "\n",
    "    # Apply on the model itself.\n",
    "    model = fsdp_wrapper(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size: int = 1):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    # Tokenize the dataset.\n",
    "    def _tokenize(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_batch(batch_size=1, sequence_length=128):\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    input_ids = torch.ones(batch_size, sequence_length, dtype=torch.long).to(device)\n",
    "    attention_mask = torch.ones(batch_size, sequence_length, dtype=torch.long).to(device)\n",
    "    labels = torch.ones(batch_size, sequence_length, dtype=torch.long).to(device)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    # dist.init_process_group('xla', init_method='xla://')\n",
    "    torch.manual_seed(99)\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    model, tokenizer = init_model(model_name=MODEL_NAME)\n",
    "    model = apply_lora(model)\n",
    "    model = apply_fsdp(model)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    train_dataloader, test_dataloader = get_dataset(tokenizer=tokenizer, batch_size=1)\n",
    "    train_dataloader, test_dataloader = pl.MpDeviceLoader(train_dataloader, device), pl.MpDeviceLoader(test_dataloader, device)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        xm.master_print(f'Epoch {epoch} train begin {test_utils.now()}')\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # labels = batch['input_ids'].clone()\n",
    "            # labels[:, :-1] = batch['input_ids'][:, 1:]\n",
    "            # labels[:, -1] = -100\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            output = model(input_ids=batch['input_ids'],\n",
    "                           attention_mask=batch['attention_mask'],\n",
    "                           labels=batch['input_ids'])\n",
    "            loss = output.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            xm.master_print(f'Loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     xmp.spawn(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721519613.360130   46116 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "I0000 00:00:1721519613.360132   46115 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721519613.360974   46118 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721519613.361216   46117 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1721519614.444494   46116 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n",
      "I0000 00:00:1721519614.449798   46115 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n",
      "I0000 00:00:1721519614.450647   46118 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n",
      "I0000 00:00:1721519614.569632   46117 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n",
      "trainable params: 1,126,400 || all params: 1,101,174,784 || trainable%: 0.10229075496156657\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   2%|▏         | 1000/43996 [00:00<00:30, 1407.14 examples/s]\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]1 examples/s]\n",
      "Map:   7%|▋         | 3000/43996 [00:02<00:35, 1161.74 examples/s]\n",
      "Map:   5%|▍         | 2000/43996 [00:01<00:25, 1637.92 examples/s][A\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:   7%|▋         | 3000/43996 [00:02<00:29, 1369.12 examples/s][A\n",
      "Map:   2%|▏         | 1000/43996 [00:01<00:44, 959.33 examples/s]\u001b[A\n",
      "Map:   2%|▏         | 1000/43996 [00:00<00:42, 1007.74 examples/s]\u001b[A\n",
      "Map:   2%|▏         | 1000/43996 [00:01<01:00, 708.69 examples/s]\u001b[A\n",
      "Map:   7%|▋         | 3000/43996 [00:02<00:40, 1003.03 examples/s]\u001b[A\n",
      "Map:   5%|▍         | 2000/43996 [00:02<00:45, 915.16 examples/s]\u001b[A\n",
      "Map:   5%|▍         | 2000/43996 [00:02<00:42, 993.07 examples/s] \u001b[A\n",
      "Map:  11%|█▏        | 5000/43996 [00:04<00:33, 1149.77 examples/s][A\n",
      "Map:  14%|█▎        | 6000/43996 [00:05<00:38, 982.47 examples/s] [A\n",
      "Map:   7%|▋         | 3000/43996 [00:03<00:43, 944.50 examples/s]\u001b[A\n",
      "Map:   7%|▋         | 3000/43996 [00:03<00:41, 986.45 examples/s]\u001b[A\n",
      "Map:  11%|█▏        | 5000/43996 [00:04<00:39, 985.05 examples/s] [A\n",
      "Map:  16%|█▌        | 7000/43996 [00:06<00:37, 983.54 examples/s]\u001b[A\n",
      "Map:   9%|▉         | 4000/43996 [00:04<00:43, 924.07 examples/s]\u001b[A\n",
      "Map:  16%|█▌        | 7000/43996 [00:06<00:36, 1002.56 examples/s][A\n",
      "Map:  18%|█▊        | 8000/43996 [00:07<00:35, 1004.31 examples/s][A\n",
      "Map:  14%|█▎        | 6000/43996 [00:06<00:39, 971.80 examples/s]\u001b[A\n",
      "Map:  11%|█▏        | 5000/43996 [00:05<00:41, 942.40 examples/s]\u001b[A\n",
      "Map:  11%|█▏        | 5000/43996 [00:05<00:39, 985.23 examples/s]\u001b[A\n",
      "Map:  18%|█▊        | 8000/43996 [00:07<00:37, 956.07 examples/s] [A\n",
      "Map:  14%|█▎        | 6000/43996 [00:06<00:40, 935.71 examples/s]\u001b[A\n",
      "Map:  14%|█▎        | 6000/43996 [00:06<00:39, 954.08 examples/s]\u001b[A\n",
      "Map:  23%|██▎       | 10000/43996 [00:09<00:31, 1080.10 examples/s]A\n",
      "Map:  20%|██        | 9000/43996 [00:08<00:36, 947.64 examples/s]\u001b[A\n",
      "Map:  16%|█▌        | 7000/43996 [00:07<00:39, 945.05 examples/s]\u001b[A\n",
      "Map:  16%|█▌        | 7000/43996 [00:07<00:38, 959.33 examples/s]\u001b[A\n",
      "Map:  23%|██▎       | 10000/43996 [00:09<00:37, 901.59 examples/s]]A\n",
      "Map:  16%|█▌        | 7000/43996 [00:08<00:43, 852.69 examples/s]\u001b[A\n",
      "Map:  20%|██        | 9000/43996 [00:09<00:38, 912.44 examples/s]\u001b[A\n",
      "Map:  18%|█▊        | 8000/43996 [00:08<00:38, 934.93 examples/s]\u001b[A\n",
      "Map:  23%|██▎       | 10000/43996 [00:10<00:36, 929.34 examples/s] A\n",
      "Map:  25%|██▌       | 11000/43996 [00:10<00:37, 883.30 examples/s][A\n",
      "Map:  20%|██        | 9000/43996 [00:09<00:38, 919.77 examples/s]\u001b[A\n",
      "Map:  30%|██▉       | 13000/43996 [00:12<00:31, 987.04 examples/s][A\n",
      "Map:  23%|██▎       | 10000/43996 [00:10<00:36, 923.45 examples/s]\u001b[A\n",
      "Map:  25%|██▌       | 11000/43996 [00:11<00:38, 861.01 examples/s]][A\n",
      "Map:  23%|██▎       | 10000/43996 [00:11<00:39, 850.94 examples/s]\u001b[A\n",
      "\n",
      "Map:  27%|██▋       | 12000/43996 [00:12<00:36, 865.44 examples/s]\u001b[A\u001b[A\n",
      "Map:  34%|███▍      | 15000/43996 [00:14<00:30, 964.28 examples/s] [A\n",
      "Map:  25%|██▌       | 11000/43996 [00:12<00:38, 863.82 examples/s]\u001b[A\n",
      "Map:  25%|██▌       | 11000/43996 [00:11<00:38, 866.38 examples/s]\u001b[A\n",
      "Map:  27%|██▋       | 12000/43996 [00:13<00:36, 883.70 examples/s]\u001b[A\n",
      "Map:  36%|███▋      | 16000/43996 [00:15<00:29, 954.96 examples/s]\u001b[A\n",
      "Map:  30%|██▉       | 13000/43996 [00:14<00:36, 854.96 examples/s]\u001b[A\n",
      "Map:  32%|███▏      | 14000/43996 [00:14<00:35, 844.91 examples/s]\u001b[A\n",
      "Map:  30%|██▉       | 13000/43996 [00:14<00:33, 929.59 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:16<00:29, 914.34 examples/s]\u001b[A\n",
      "Map:  32%|███▏      | 14000/43996 [00:15<00:35, 844.88 examples/s]\u001b[A\n",
      "Map:  30%|██▉       | 13000/43996 [00:14<00:36, 841.29 examples/s]\u001b[A\n",
      "Map:  27%|██▋       | 12000/43996 [00:14<00:37, 846.82 examples/s]\u001b[A\n",
      "Map:  36%|███▋      | 16000/43996 [00:16<00:31, 895.72 examples/s]\u001b[A\n",
      "Map:  34%|███▍      | 15000/43996 [00:16<00:32, 888.50 examples/s]\u001b[A\n",
      "Map:  32%|███▏      | 14000/43996 [00:15<00:34, 858.98 examples/s]\u001b[A\n",
      "Map:  43%|████▎     | 19000/43996 [00:18<00:26, 949.74 examples/s]\u001b[A\n",
      "Map:  36%|███▋      | 16000/43996 [00:17<00:29, 945.98 examples/s]\u001b[A\n",
      "Map:  36%|███▋      | 16000/43996 [00:17<00:31, 886.72 examples/s]\u001b[A\n",
      "Map:  34%|███▍      | 15000/43996 [00:16<00:32, 893.19 examples/s]\u001b[A\n",
      "Map:  45%|████▌     | 20000/43996 [00:19<00:24, 964.11 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:18<00:28, 960.96 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:18<00:29, 911.36 examples/s]\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:18<00:29, 877.87 examples/s]\u001b[A\n",
      "Map:  48%|████▊     | 21000/43996 [00:20<00:23, 984.59 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:18<00:29, 913.93 examples/s]\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:19<00:27, 936.92 examples/s]\u001b[A\n",
      "Map:  43%|████▎     | 19000/43996 [00:19<00:28, 886.30 examples/s]\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:21<00:22, 979.59 examples/s]\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:19<00:28, 927.03 examples/s]][A\n",
      "Map:  41%|████      | 18000/43996 [00:19<00:28, 926.65 examples/s]\u001b[A\n",
      "Map:  45%|████▌     | 20000/43996 [00:20<00:26, 904.59 examples/s]\u001b[A\n",
      "Map:  52%|█████▏    | 23000/43996 [00:22<00:21, 969.95 examples/s]\u001b[A\n",
      "Map:  43%|████▎     | 19000/43996 [00:20<00:26, 937.00 examples/s] [A\n",
      "Map:  43%|████▎     | 19000/43996 [00:20<00:26, 938.81 examples/s]\u001b[A\n",
      "Map:  48%|████▊     | 21000/43996 [00:22<00:24, 927.33 examples/s]\u001b[A\n",
      "Map:  55%|█████▍    | 24000/43996 [00:23<00:20, 976.80 examples/s]\u001b[A\n",
      "Map:  45%|████▌     | 20000/43996 [00:21<00:25, 946.59 examples/s]\u001b[A\n",
      "Map:  45%|████▌     | 20000/43996 [00:21<00:25, 946.29 examples/s]\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:23<00:24, 912.76 examples/s]\u001b[A\n",
      "Map:  57%|█████▋    | 25000/43996 [00:24<00:19, 964.17 examples/s]\u001b[A\n",
      "Map:  48%|████▊     | 21000/43996 [00:22<00:23, 968.72 examples/s]\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:23<00:24, 893.49 examples/s]\u001b[A\n",
      "Map:  52%|█████▏    | 23000/43996 [00:24<00:22, 928.24 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:26<00:19, 905.46 examples/s]\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:23<00:24, 908.10 examples/s]\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:23<00:24, 907.73 examples/s]\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:24<00:23, 945.17 examples/s]\u001b[A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:27<00:17, 951.83 examples/s]\u001b[A\n",
      "Map:  55%|█████▍    | 24000/43996 [00:25<00:21, 940.36 examples/s]\u001b[A\n",
      "Map:  52%|█████▏    | 23000/43996 [00:24<00:23, 907.75 examples/s]\u001b[A\n",
      "Map:  57%|█████▋    | 25000/43996 [00:26<00:21, 884.12 examples/s]\u001b[A\n",
      "Map:  55%|█████▍    | 24000/43996 [00:25<00:20, 962.54 examples/s]\u001b[A\n",
      "Map:  57%|█████▋    | 25000/43996 [00:27<00:20, 916.25 examples/s]\u001b[A\n",
      "Map:  57%|█████▋    | 25000/43996 [00:26<00:20, 930.25 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:27<00:20, 872.89 examples/s]\u001b[A\n",
      "Map:  66%|██████▌   | 29000/43996 [00:29<00:16, 903.96 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:28<00:19, 934.48 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:27<00:18, 955.54 examples/s]\u001b[A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:28<00:18, 912.91 examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 30000/43996 [00:30<00:14, 934.36 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:27<00:18, 951.67 examples/s]\u001b[A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:28<00:17, 974.87 examples/s]\u001b[A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:29<00:17, 899.08 examples/s]\u001b[A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:29<00:15, 1005.31 examples/s][A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:29<00:17, 952.62 examples/s]\u001b[A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:28<00:17, 964.51 examples/s]\u001b[A\n",
      "Map:  66%|██████▌   | 29000/43996 [00:30<00:16, 923.60 examples/s]\u001b[A\n",
      "Map:  73%|███████▎  | 32000/43996 [00:32<00:12, 974.55 examples/s]\u001b[A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:30<00:16, 948.20 examples/s] [A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:29<00:16, 955.94 examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 30000/43996 [00:31<00:14, 944.05 examples/s]\u001b[A\n",
      "Map:  75%|███████▌  | 33000/43996 [00:33<00:11, 961.45 examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 30000/43996 [00:31<00:14, 979.71 examples/s]\u001b[A\n",
      "Map:  66%|██████▌   | 29000/43996 [00:30<00:15, 953.80 examples/s]\u001b[A\n",
      "Map:  70%|███████   | 31000/43996 [00:32<00:13, 979.59 examples/s]\u001b[A\n",
      "Map:  70%|███████   | 31000/43996 [00:32<00:12, 1008.43 examples/s][A\n",
      "Map:  70%|███████   | 31000/43996 [00:33<00:13, 967.23 examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 30000/43996 [00:32<00:14, 941.02 examples/s]\u001b[A\n",
      "Map:  73%|███████▎  | 32000/43996 [00:33<00:12, 942.66 examples/s]\u001b[A\n",
      "Map:  70%|███████   | 31000/43996 [00:33<00:13, 974.09 examples/s] [A\n",
      "Map:  73%|███████▎  | 32000/43996 [00:34<00:12, 942.53 examples/s]\u001b[A\n",
      "\n",
      "Map:  75%|███████▌  | 33000/43996 [00:35<00:11, 928.06 examples/s]\u001b[A\u001b[A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:36<00:08, 992.20 examples/s]\u001b[A\n",
      "Map:  75%|███████▌  | 33000/43996 [00:35<00:11, 945.14 examples/s]\u001b[A\n",
      "Map:  73%|███████▎  | 32000/43996 [00:34<00:12, 957.44 examples/s]\n",
      "Map:  77%|███████▋  | 34000/43996 [00:36<00:10, 932.89 examples/s]\u001b[A\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:37<00:07, 950.68 examples/s]\u001b[A\n",
      "Map:  75%|███████▌  | 33000/43996 [00:35<00:10, 1000.60 examples/s]\u001b[A\n",
      "Map:  77%|███████▋  | 34000/43996 [00:36<00:11, 877.61 examples/s]\u001b[A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:37<00:09, 963.83 examples/s]\u001b[A\n",
      "Map:  86%|████████▋ | 38000/43996 [00:38<00:06, 922.81 examples/s]\u001b[A\n",
      "Map:  77%|███████▋  | 34000/43996 [00:36<00:11, 884.69 examples/s] [A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:37<00:10, 871.62 examples/s]\u001b[A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:38<00:08, 973.48 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:39<00:05, 934.02 examples/s]\u001b[A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:37<00:10, 871.67 examples/s]\u001b[A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:38<00:09, 879.60 examples/s]\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:39<00:07, 933.88 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:40<00:04, 907.93 examples/s]\u001b[A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:38<00:08, 906.01 examples/s]\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:39<00:07, 914.49 examples/s]\u001b[A\n",
      "Map:  86%|████████▋ | 38000/43996 [00:40<00:06, 976.84 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:41<00:03, 925.44 examples/s]\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:39<00:07, 936.11 examples/s]\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:39<00:07, 949.22 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:41<00:05, 908.99 examples/s]\u001b[A\n",
      "Map:  95%|█████████▌| 42000/43996 [00:42<00:02, 941.02 examples/s]\u001b[A\n",
      "Map:  86%|████████▋ | 38000/43996 [00:40<00:06, 954.07 examples/s]][A\n",
      "Map:  86%|████████▋ | 38000/43996 [00:40<00:06, 963.99 examples/s]\u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:43<00:01, 978.17 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:42<00:04, 955.07 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:41<00:05, 952.88 examples/s] [A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:41<00:05, 961.58 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:45<00:00, 936.08 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:43<00:03, 954.42 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:42<00:04, 940.19 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:42<00:04, 944.35 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:45<00:00, 964.50 examples/s]\u001b[A\n",
      "\n",
      "Map:  95%|█████████▌| 42000/43996 [00:44<00:02, 871.26 examples/s]]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:43<00:03, 954.92 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:43<00:03, 958.08 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:44<00:03, 974.22 examples/s]\u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:46<00:01, 886.97 examples/s]]\u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:45<00:01, 934.71 examples/s]\u001b[A\n",
      "\n",
      "Map:  95%|█████████▌| 42000/43996 [00:45<00:02, 978.04 examples/s]\u001b[A\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:47<00:00, 933.79 examples/s] \u001b[A\n",
      "Map:  39%|███▊      | 3000/7764 [00:02<00:03, 1197.51 examples/s]\n",
      "Map: 100%|██████████| 43996/43996 [00:47<00:00, 899.85 examples/s]\n",
      "Map:  98%|█████████▊| 43000/43996 [00:45<00:01, 948.35 examples/s]\u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:45<00:01, 961.75 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:47<00:00, 921.70 examples/s]\u001b[A\n",
      "\n",
      "Map:   0%|          | 0/7764 [00:00<?, ? examples/s].96 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:47<00:00, 923.68 examples/s]]\u001b[A\n",
      "Map:  52%|█████▏    | 4000/7764 [00:03<00:03, 1081.07 examples/s]\n",
      "Map: 100%|██████████| 43996/43996 [00:46<00:00, 948.74 examples/s]A\n",
      "Map: 100%|██████████| 43996/43996 [00:47<00:00, 919.97 examples/s] [A\n",
      "\n",
      "Map:  13%|█▎        | 1000/7764 [00:01<00:06, 973.97 examples/s]/s]\u001b[A\n",
      "Map:  64%|██████▍   | 5000/7764 [00:04<00:02, 1027.81 examples/s]\n",
      "Map: 100%|██████████| 43996/43996 [00:48<00:00, 910.54 examples/s]A\n",
      "Map: 100%|██████████| 43996/43996 [00:48<00:00, 907.87 examples/s]\n",
      "Map:  26%|██▌       | 2000/7764 [00:02<00:06, 906.06 examples/s]s] \u001b[A\n",
      "\n",
      "Map:  77%|███████▋  | 6000/7764 [00:05<00:01, 979.81 examples/s] \n",
      "Map:  39%|███▊      | 3000/7764 [00:02<00:04, 1078.74 examples/s][A\n",
      "Map: 100%|██████████| 43996/43996 [00:49<00:00, 894.63 examples/s]\n",
      "\u001b[A\n",
      "Map:  26%|██▌       | 2000/7764 [00:01<00:04, 1175.39 examples/s]\n",
      "Map:  90%|█████████ | 7000/7764 [00:06<00:00, 966.42 examples/s]\u001b[A\n",
      "Map:  52%|█████▏    | 4000/7764 [00:03<00:03, 1021.75 examples/s][A\n",
      "Map:  52%|█████▏    | 4000/7764 [00:04<00:03, 950.28 examples/s] [A\n",
      "Map:  39%|███▊      | 3000/7764 [00:02<00:04, 1092.26 examples/s][A\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 974.35 examples/s]\u001b[A\n",
      "Map:  64%|██████▍   | 5000/7764 [00:04<00:02, 1087.01 examples/s]\n",
      "Map:  64%|██████▍   | 5000/7764 [00:05<00:02, 965.63 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 3000/7764 [00:03<00:05, 917.88 examples/s]\u001b[A\n",
      "Map:  64%|██████▍   | 5000/7764 [00:05<00:02, 948.07 examples/s]]\u001b[A\n",
      "Map:  26%|██▌       | 2000/7764 [00:02<00:06, 845.43 examples/s]\u001b[A\n",
      "Map:  90%|█████████ | 7000/7764 [00:06<00:00, 1334.80 examples/s]\u001b[A\n",
      "Map:  77%|███████▋  | 6000/7764 [00:05<00:01, 1008.90 examples/s][A\n",
      "Map:  52%|█████▏    | 4000/7764 [00:04<00:04, 934.76 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:06<00:00, 1131.65 examples/s]\u001b[A\n",
      "Map:  77%|███████▋  | 6000/7764 [00:06<00:01, 985.28 examples/s]\n",
      "Map:  90%|█████████ | 7000/7764 [00:06<00:00, 1025.51 examples/s][A\n",
      "Map:  64%|██████▍   | 5000/7764 [00:04<00:02, 979.94 examples/s]\u001b[A\n",
      "Map:  90%|█████████ | 7000/7764 [00:07<00:00, 1015.40 examples/s][A\n",
      "Map:  52%|█████▏    | 4000/7764 [00:04<00:04, 921.03 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 929.46 examples/s] [A\n",
      "Map: 100%|██████████| 7764/7764 [00:08<00:00, 931.95 examples/s] [A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 23:56:41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 7764/7764 [00:08<00:00, 938.54 examples/s]\n",
      "\n",
      "Map:  64%|██████▍   | 5000/7764 [00:05<00:02, 948.31 examples/s]\u001b[A\n",
      "Map:  90%|█████████ | 7000/7764 [00:06<00:00, 1146.57 examples/s]\u001b[A\n",
      "Map:  90%|█████████ | 7000/7764 [00:06<00:00, 973.97 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 1232.60 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 1032.58 examples/s]\u001b[A\n",
      "\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 923.17 examples/s]]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 978.93 examples/s]\u001b[A\n",
      "\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 1087.86 examples/s]\u001b[A\n",
      "\n",
      "Map: 100%|██████████| 7764/7764 [00:07<00:00, 994.40 examples/s] \u001b[A\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "'weight' must be 2-D",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 83, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 75, in _run_thread_per_device\n    replica_results = list(\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\n    yield _result_or_cancel(fs.pop())\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\n    return fut.result(timeout)\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n    return self.__get_result()\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 68, in _thread_fn\n    return fn()\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 178, in __call__\n    self.fn(runtime.global_ordinal(), *self.args, **self.kwargs)\n  File \"/tmp/ipykernel_45971/1143278732.py\", line 26, in train\n    output = model(input_ids=batch['input_ids'],\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/distributed/fsdp/xla_fully_sharded_data_parallel.py\", line 937, in forward\n    outputs = self.module(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/distributed/fsdp/xla_flatten_params_wrapper.py\", line 525, in forward\n    return self.module(*inputs, **kwinputs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/peft/peft_model.py\", line 1129, in forward\n    return self.base_model(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/peft/tuners/tuners_utils.py\", line 161, in forward\n    return self.model.forward(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 1176, in forward\n    outputs = self.model(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/transformers/models/llama/modeling_llama.py\", line 977, in forward\n    inputs_embeds = self.embed_tokens(input_ids)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n    return forward_call(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n    return F.embedding(\n  File \"/usr/local/lib/python3.10/site-packages/torch/nn/functional.py\", line 2233, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n  File \"/usr/local/lib/python3.10/site-packages/torch/_decomp/decompositions.py\", line 1132, in embedding\n    assert weight.dim() == 2, \"'weight' must be 2-D\"\nAssertionError: 'weight' must be 2-D\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:83\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     81\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:202\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 202\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:83\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     81\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:159\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    154\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    155\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    156\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    157\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    158\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[0;32m--> 159\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m      \u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m          \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess_results\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m runtime\u001b[38;5;241m.\u001b[39mdevice_type() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    164\u001b[0m   gpu\u001b[38;5;241m.\u001b[39mshutdown_distributed_runtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:160\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    154\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    155\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    156\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    157\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    158\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[1;32m    159\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m--> 160\u001b[0m       itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m    161\u001b[0m           result\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m process_results))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m runtime\u001b[38;5;241m.\u001b[39mdevice_type() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    164\u001b[0m   gpu\u001b[38;5;241m.\u001b[39mshutdown_distributed_runtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mAssertionError\u001b[0m: 'weight' must be 2-D"
     ]
    }
   ],
   "source": [
    "xmp.spawn(train, start_method=\"fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 388.410448,
     "end_time": "2023-11-04T13:21:54.038795",
     "exception": false,
     "start_time": "2023-11-04T13:15:25.628347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = model.cpu()\n",
    "# print('now saving the model')\n",
    "# model.push_to_hub(\n",
    "#     \"felarof01/llama3-test\", \n",
    "#     tokenizer=tokenizer,\n",
    "#     private=False,\n",
    "#     create_pr=False,\n",
    "#     max_shard_size=\"2GB\", # Sharding isn't as important as before since hardware is better now but who cares anyway\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3555678,
     "isSourceIdPinned": true,
     "sourceId": 6196932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3936750,
     "sourceId": 6847931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946973,
     "sourceId": 6867914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937441,
     "sourceId": 6868189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949797,
     "sourceId": 6873567,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3944051,
     "sourceId": 7060310,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30529,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
