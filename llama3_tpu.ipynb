{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 121.512757,
     "end_time": "2023-11-04T12:34:07.401258",
     "exception": false,
     "start_time": "2023-11-04T12:32:05.888501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !pip install --upgrade pip -q\n",
    "# !pip install transformers datasets sentencepiece peft -q\n",
    "# !pip install huggingface_hub -q\n",
    "# !pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!export USE_TORCH=True # To use transformers library in TPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "papermill": {
     "duration": 13.608021,
     "end_time": "2023-11-04T12:34:21.013846",
     "exception": false,
     "start_time": "2023-11-04T12:34:07.405825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.distributed as dist\n",
    "import torch_xla.core.xla_model as xm\n",
    "from torch_xla.distributed.fsdp import XlaFullyShardedDataParallel as FSDP, checkpoint_module\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from peft import LoraConfig, TaskType, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.fsdp' from '/home/tunerX/utils/fsdp.py'>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import importlib\n",
    "sys.path.append('')\n",
    "fsdp_util = importlib.import_module('utils.fsdp')\n",
    "importlib.reload(fsdp_util)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"TinyLlama/TinyLlama-1.1B-step-50K-105b\"\n",
    "# \"meta-llama/Meta-Llama-3-8B\" \n",
    "# \"meta-llama/Llama-2-7b-hf\" \n",
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model(*, model_name):\n",
    "    config = AutoConfig.from_pretrained(model_name, use_auth_token=True)\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    if not tokenizer.pad_token:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    return model, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lora(model):\n",
    "    # TODO: pass lora config as argument to function.\n",
    "    peft_config = LoraConfig(task_type=TaskType.CAUSAL_LM,\n",
    "                             inference_mode=False,\n",
    "                             r=8,\n",
    "                             lora_alpha=32,\n",
    "                             lora_dropout=0.1)\n",
    "    model = get_peft_model(model, peft_config)\n",
    "    model.print_trainable_parameters()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fsdp_wrapper(x):\n",
    "    return FSDP(x, shard_param_on_dim_0=True, pin_layout_in_collective_ops=True, disable_reshard_on_root=False, reshard_after_forward=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "papermill": {
     "duration": 0.547357,
     "end_time": "2023-11-04T12:34:26.034497",
     "exception": false,
     "start_time": "2023-11-04T12:34:25.48714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def apply_fsdp(model):\n",
    "    # Apply on layers within model.\n",
    "    fsdp_util.apply_fsdp(model, [\"LlamaDecoderLayer\"])\n",
    "\n",
    "    # Apply on the model itself.\n",
    "    model = fsdp_wrapper(model)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(*, tokenizer, batch_size: int = 1):\n",
    "    # Define Alpaca prompt template\n",
    "    alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "    \n",
    "    ### Instruction: {}\n",
    "    \n",
    "    ### Input: {}\n",
    "    \n",
    "    ### Response: {}\"\"\"\n",
    "    \n",
    "    EOS_TOKEN = tokenizer.eos_token\n",
    "    \n",
    "    # Define formatting function.\n",
    "    def _format_prompts(examples):\n",
    "        instructions = examples[\"instruction\"]\n",
    "        inputs = examples[\"input\"]\n",
    "        outputs = examples[\"output\"]\n",
    "        texts = []\n",
    "        for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "            text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "            texts.append(text)\n",
    "        return {\"text\": texts}\n",
    "\n",
    "    # Tokenize the dataset.\n",
    "    def _tokenize(examples):\n",
    "        return tokenizer(examples[\"text\"], truncation=True, padding=\"max_length\", max_length=512)\n",
    "\n",
    "    # Load and preprocess the dataset.\n",
    "    dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "    dataset = dataset.map(_format_prompts, batched=True)\n",
    "\n",
    "    # Create train and test dataset.\n",
    "    ds = dataset.train_test_split(test_size=0.15)\n",
    "    ds['train'] = ds['train'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "    ds['test'] = ds['test'].map(_tokenize, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "    # Create DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['train'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "    \n",
    "    test_dataloader = torch.utils.data.DataLoader(\n",
    "        ds['test'],\n",
    "        shuffle=True,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=default_data_collator,\n",
    "    )\n",
    "\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_batch(batch_size=1, sequence_length=128):\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    input_ids = torch.ones(batch_size, sequence_length, dtype=torch.long).to(device)\n",
    "    attention_mask = torch.ones(batch_size, sequence_length, dtype=torch.long).to(device)\n",
    "    labels = torch.ones(batch_size, sequence_length, dtype=torch.long).to(device)\n",
    "    \n",
    "    return {\n",
    "        'input_ids': input_ids,\n",
    "        'attention_mask': attention_mask,\n",
    "        'labels': labels\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(index):\n",
    "    # dist.init_process_group('xla', init_method='xla://')\n",
    "    torch.manual_seed(99)\n",
    "    device = xm.xla_device()\n",
    "    \n",
    "    model, tokenizer = init_model(model_name=MODEL_NAME)\n",
    "    model = apply_lora(model)\n",
    "    model = apply_fsdp(model)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "    \n",
    "    train_dataloader, test_dataloader = get_dataset(tokenizer=tokenizer, batch_size=1)\n",
    "    train_dataloader, test_dataloader = pl.MpDeviceLoader(train_dataloader, device), pl.MpDeviceLoader(test_dataloader, device)\n",
    "    \n",
    "    for epoch in range(1):\n",
    "        xm.master_print(f'Epoch {epoch} train begin {test_utils.now()}')\n",
    "        \n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # labels = batch['input_ids'].clone()\n",
    "            # labels[:, :-1] = batch['input_ids'][:, 1:]\n",
    "            # labels[:, -1] = -100\n",
    "            # batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            \n",
    "            output = model(input_ids=batch['input_ids'],\n",
    "                           attention_mask=batch['attention_mask'],\n",
    "                           labels=batch['input_ids'])\n",
    "            loss = output.loss\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            xm.master_print(f'Loss: {loss:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if __name__ == '__main__':\n",
    "#     xmp.spawn(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721519999.904513   51782 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721519999.905489   51783 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721519999.907571   51784 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721519999.911000   51785 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/transformers/models/auto/configuration_auto.py:1096: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.10/site-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "I0000 00:00:1721520002.188099   51783 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n",
      "I0000 00:00:1721520002.195237   51782 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n",
      "I0000 00:00:1721520002.196222   51785 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n",
      "I0000 00:00:1721520002.343996   51784 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n",
      "trainable params: 3,407,872 || all params: 8,033,669,120 || trainable%: 0.04241987003816259\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:  99%|█████████▊| 51000/51760 [00:01<00:00, 45705.18 examples/s]\n",
      "Map:   0%|          | 0/51760 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:  12%|█▏        | 6000/51760 [00:00<00:00, 47404.34 examples/s]\u001b[A\n",
      "Map:  23%|██▎       | 12000/51760 [00:00<00:00, 49469.87 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 51760/51760 [00:02<00:00, 21869.50 examples/s]\u001b[A\n",
      "\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s].58 examples/s]\u001b[A\n",
      "Map:  58%|█████▊    | 30000/51760 [00:01<00:01, 21197.99 examples/s]\u001b[A\n",
      "Map:   2%|▏         | 1000/43996 [00:00<00:31, 1346.25 examples/s]s]\u001b[A\n",
      "Map:  71%|███████▏  | 37000/51760 [00:01<00:01, 14461.18 examples/s]\u001b[A\n",
      "Map:   5%|▍         | 2000/43996 [00:01<00:30, 1392.09 examples/s]s]\u001b[A\n",
      "Map:  83%|████████▎ | 43000/51760 [00:02<00:00, 11667.08 examples/s]\u001b[A\n",
      "Map:   7%|▋         | 3000/43996 [00:02<00:30, 1362.46 examples/s]] \u001b[A\n",
      "Map:  93%|█████████▎| 48000/51760 [00:03<00:00, 10218.76 examples/s]\u001b[A\n",
      "Map:   9%|▉         | 4000/43996 [00:02<00:29, 1375.75 examples/s]] \u001b[A\n",
      "Map: 100%|██████████| 51760/51760 [00:04<00:00, 11385.99 examples/s][A\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:   2%|▏         | 1000/43996 [00:01<00:50, 854.76 examples/s]]\n",
      "Map:   5%|▍         | 2000/43996 [00:02<00:43, 955.24 examples/s]][A\n",
      "Map:   2%|▏         | 1000/43996 [00:01<00:53, 800.53 examples/s]\u001b[A\n",
      "Map:   7%|▋         | 3000/43996 [00:03<00:46, 880.53 examples/s] [A\n",
      "Map:   5%|▍         | 2000/43996 [00:02<00:54, 773.71 examples/s]\u001b[A\n",
      "Map:   9%|▉         | 4000/43996 [00:04<00:48, 826.26 examples/s]\u001b[A\n",
      "Map:   7%|▋         | 3000/43996 [00:03<00:51, 794.05 examples/s]\u001b[A\n",
      "Map:  11%|█▏        | 5000/43996 [00:06<00:49, 789.94 examples/s]][A\n",
      "Map:  25%|██▌       | 11000/43996 [00:11<00:42, 784.49 examples/s][A\n",
      "Map:  14%|█▎        | 6000/43996 [00:07<00:48, 783.55 examples/s]\u001b[A\n",
      "Map:  16%|█▌        | 7000/43996 [00:08<00:48, 764.72 examples/s]\u001b[A\n",
      "\n",
      "Map:  18%|█▊        | 8000/43996 [00:10<00:46, 771.65 examples/s]][A\u001b[A\n",
      "Map:  16%|█▌        | 7000/43996 [00:09<00:49, 742.21 examples/s]\n",
      "Map:  32%|███▏      | 14000/43996 [00:15<00:39, 756.23 examples/s][A\n",
      "Map:  18%|█▊        | 8000/43996 [00:11<00:49, 730.85 examples/s]\n",
      "Map:  23%|██▎       | 10000/43996 [00:12<00:45, 740.57 examples/s][A\n",
      "Map:  20%|██        | 9000/43996 [00:11<00:46, 745.24 examples/s]\u001b[A\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:  25%|██▌       | 11000/43996 [00:14<00:43, 750.02 examples/s][A\n",
      "Map:   2%|▏         | 1000/43996 [00:01<01:16, 564.32 examples/s]]\u001b[A\n",
      "\n",
      "Map:  27%|██▋       | 12000/43996 [00:15<00:42, 747.30 examples/s]\u001b[A\u001b[A\n",
      "Map:   5%|▍         | 2000/43996 [00:03<01:03, 660.30 examples/s]]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:19<00:37, 727.81 examples/s]\n",
      "Map:   7%|▋         | 3000/43996 [00:04<01:02, 654.23 examples/s]][A\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:21<00:36, 716.53 examples/s]\u001b[A\n",
      "Map:  30%|██▉       | 13000/43996 [00:17<00:45, 684.95 examples/s]\u001b[A\n",
      "Map:  32%|███▏      | 14000/43996 [00:18<00:43, 691.52 examples/s][A\n",
      "Map:  43%|████▎     | 19000/43996 [00:22<00:35, 708.30 examples/s]\u001b[A\n",
      "Map:   9%|▉         | 4000/43996 [00:06<01:02, 644.04 examples/s]]\n",
      "Map:   9%|▉         | 4000/43996 [00:05<00:57, 697.13 examples/s]\u001b[A\n",
      "Map:  34%|███▍      | 15000/43996 [00:20<00:41, 692.46 examples/s]\u001b[A\n",
      "Map:  32%|███▏      | 14000/43996 [00:19<00:42, 701.40 examples/s]\u001b[A\n",
      "Map:  14%|█▎        | 6000/43996 [00:09<00:56, 668.58 examples/s]\u001b[A\n",
      "Map:  48%|████▊     | 21000/43996 [00:25<00:33, 678.96 examples/s][A\n",
      "\n",
      "Map:  16%|█▌        | 7000/43996 [00:10<00:50, 737.77 examples/s]]\u001b[A\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:26<00:31, 709.10 examples/s][A\n",
      "Map:  39%|███▊      | 17000/43996 [00:22<00:38, 694.14 examples/s]\u001b[A\n",
      "Map:  18%|█▊        | 8000/43996 [00:11<00:47, 765.29 examples/s]]\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:24<00:36, 719.42 examples/s][A\n",
      "Map:  39%|███▊      | 17000/43996 [00:23<00:37, 717.08 examples/s]\u001b[A\n",
      "Map:  20%|██        | 9000/43996 [00:12<00:42, 814.00 examples/s]]\u001b[A\n",
      "Map:  55%|█████▍    | 24000/43996 [00:29<00:27, 733.07 examples/s][A\n",
      "Map:  41%|████      | 18000/43996 [00:24<00:35, 733.98 examples/s]\u001b[A\n",
      "Map:  23%|██▎       | 10000/43996 [00:13<00:42, 803.65 examples/s]\u001b[A\n",
      "Map:  57%|█████▋    | 25000/43996 [00:30<00:24, 762.10 examples/s]\u001b[A\n",
      "Map:  43%|████▎     | 19000/43996 [00:25<00:32, 759.52 examples/s]\u001b[A\n",
      "Map:  25%|██▌       | 11000/43996 [00:15<00:41, 786.51 examples/s]\u001b[A\n",
      "Map:  25%|██▌       | 11000/43996 [00:14<00:42, 781.79 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:32<00:25, 711.95 examples/s]\u001b[A\n",
      "Map:  50%|█████     | 22000/43996 [00:28<00:27, 788.47 examples/s]\u001b[A\n",
      "Map:  27%|██▋       | 12000/43996 [00:16<00:42, 759.02 examples/s]\u001b[A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:33<00:23, 735.91 examples/s]\u001b[A\n",
      "Map:  52%|█████▏    | 23000/43996 [00:30<00:28, 735.55 examples/s]\u001b[A\n",
      "Map:  30%|██▉       | 13000/43996 [00:18<00:43, 705.93 examples/s]\u001b[A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:34<00:21, 735.07 examples/s]\u001b[A\n",
      "Map:  32%|███▏      | 14000/43996 [00:19<00:41, 723.76 examples/s]\u001b[A\n",
      "Map:  55%|█████▍    | 24000/43996 [00:32<00:29, 686.83 examples/s]\u001b[A\n",
      "Map:  66%|██████▌   | 29000/43996 [00:36<00:21, 710.04 examples/s]\u001b[A\n",
      "Map:  34%|███▍      | 15000/43996 [00:20<00:37, 771.75 examples/s]\u001b[A\n",
      "Map:  57%|█████▋    | 25000/43996 [00:33<00:26, 725.72 examples/s]\u001b[A\n",
      "Map:   0%|          | 0/43996 [00:00<?, ? examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 30000/43996 [00:37<00:18, 743.37 examples/s]\n",
      "Map:  36%|███▋      | 16000/43996 [00:22<00:38, 732.80 examples/s]\u001b[A\u001b[A\n",
      "Map:   2%|▏         | 1000/43996 [00:01<01:11, 604.50 examples/s]]\u001b[A\n",
      "Map:   2%|▏         | 1000/43996 [00:01<00:49, 874.56 examples/s]\u001b[A\n",
      "Map:  70%|███████   | 31000/43996 [00:39<00:18, 720.20 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:23<00:35, 750.88 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:22<00:35, 769.51 examples/s]\u001b[A\n",
      "Map:   5%|▍         | 2000/43996 [00:02<00:51, 809.95 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:35<00:25, 708.14 examples/s]\n",
      "Map:  41%|████      | 18000/43996 [00:24<00:33, 780.60 examples/s]\u001b[A\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:23<00:32, 796.13 examples/s]\u001b[A\n",
      "Map:  66%|██████▌   | 29000/43996 [00:38<00:18, 794.07 examples/s][A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:37<00:23, 717.91 examples/s]\u001b[A\n",
      "\n",
      "Map:  68%|██████▊   | 30000/43996 [00:39<00:17, 810.78 examples/s]\u001b[A\u001b[A\n",
      "Map:  11%|█▏        | 5000/43996 [00:06<00:51, 762.27 examples/s]]\u001b[A\n",
      "Map:   9%|▉         | 4000/43996 [00:05<00:59, 668.65 examples/s]\u001b[A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:38<00:22, 703.70 examples/s]\u001b[A\n",
      "Map:  48%|████▊     | 21000/43996 [00:28<00:28, 801.41 examples/s]\u001b[A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:44<00:12, 722.62 examples/s]\u001b[A\n",
      "Map:  66%|██████▌   | 29000/43996 [00:39<00:20, 728.35 examples/s]\u001b[A\n",
      "Map:  11%|█▏        | 5000/43996 [00:07<00:58, 671.07 examples/s]\n",
      "Map:  50%|█████     | 22000/43996 [00:29<00:27, 788.27 examples/s]\u001b[A\u001b[A\n",
      "Map:  73%|███████▎  | 32000/43996 [00:42<00:15, 750.69 examples/s]\u001b[A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:46<00:11, 709.84 examples/s]\u001b[A\n",
      "Map:  14%|█▎        | 6000/43996 [00:08<00:55, 682.10 examples/s]\n",
      "Map:  75%|███████▌  | 33000/43996 [00:43<00:13, 813.77 examples/s]\u001b[A\u001b[A\n",
      "Map:  52%|█████▏    | 23000/43996 [00:30<00:26, 785.53 examples/s]\u001b[A\n",
      "Map:  18%|█▊        | 8000/43996 [00:10<00:48, 736.18 examples/s]]\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:47<00:10, 689.75 examples/s][A\n",
      "Map:  55%|█████▍    | 24000/43996 [00:31<00:25, 780.39 examples/s]\u001b[A\n",
      "Map:  55%|█████▍    | 24000/43996 [00:31<00:25, 775.51 examples/s]\u001b[A\n",
      "Map:  20%|██        | 9000/43996 [00:11<00:46, 750.45 examples/s]]\u001b[A\n",
      "Map:  18%|█▊        | 8000/43996 [00:11<00:49, 726.66 examples/s]]\u001b[A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:45<00:11, 796.15 examples/s]\u001b[A\n",
      "Map:  75%|███████▌  | 33000/43996 [00:44<00:14, 782.28 examples/s]\u001b[A\n",
      "Map:  23%|██▎       | 10000/43996 [00:13<00:44, 756.39 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:50<00:06, 725.92 examples/s][A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:46<00:10, 782.24 examples/s]\u001b[A\n",
      "Map:  25%|██▌       | 11000/43996 [00:14<00:41, 801.66 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:35<00:26, 673.81 examples/s]\u001b[A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:34<00:26, 682.49 examples/s]\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:48<00:09, 770.15 examples/s]\u001b[A\n",
      "Map:  27%|██▋       | 12000/43996 [00:15<00:43, 735.07 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:53<00:04, 713.19 examples/s]\u001b[A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:36<00:25, 665.91 examples/s]\u001b[A\n",
      "Map:  86%|████████▋ | 38000/43996 [00:49<00:07, 759.98 examples/s]\u001b[A\n",
      "Map:  95%|█████████▌| 42000/43996 [00:54<00:02, 745.32 examples/s]\u001b[A\n",
      "Map:  27%|██▋       | 12000/43996 [00:16<00:44, 726.05 examples/s]\u001b[A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:37<00:23, 694.71 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:51<00:06, 728.41 examples/s]\u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:55<00:01, 754.30 examples/s]\u001b[A\n",
      "Map:  66%|██████▌   | 29000/43996 [00:39<00:20, 717.80 examples/s]\u001b[A\n",
      "Map:  32%|███▏      | 14000/43996 [00:18<00:43, 693.47 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:52<00:05, 689.71 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:56<00:00, 754.15 examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 30000/43996 [00:40<00:19, 727.37 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:57<00:00, 770.53 examples/s]\u001b[A\n",
      "\n",
      "Map:  93%|█████████▎| 41000/43996 [00:54<00:04, 710.11 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:53<00:06, 731.52 examples/s]\n",
      "Map:  70%|███████   | 31000/43996 [00:41<00:17, 727.00 examples/s]\u001b[A\n",
      "Map:  36%|███▋      | 16000/43996 [00:21<00:39, 702.73 examples/s]\u001b[A\n",
      "Map:  95%|█████████▌| 42000/43996 [00:55<00:02, 737.01 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 17000/43996 [00:22<00:36, 749.65 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:55<00:05, 745.79 examples/s]\u001b[A\n",
      "Map:  73%|███████▎  | 32000/43996 [00:42<00:16, 722.37 examples/s]\u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:56<00:01, 744.93 examples/s]\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:24<00:34, 753.52 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:56<00:04, 726.74 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 3000/7764 [00:03<00:05, 818.17 examples/s]s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:57<00:00, 745.19 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:58<00:00, 753.00 examples/s]\u001b[A\n",
      "Map:  41%|████      | 18000/43996 [00:24<00:34, 748.38 examples/s]\n",
      "Map:  77%|███████▋  | 34000/43996 [00:46<00:13, 720.80 examples/s]\n",
      "Map:  77%|███████▋  | 34000/43996 [00:45<00:14, 704.13 examples/s]\u001b[A\n",
      "Map:   0%|          | 0/7764 [00:00<?, ? examples/s] examples/s]s]\u001b[A\n",
      "Map:  45%|████▌     | 20000/43996 [00:26<00:32, 730.40 examples/s]\u001b[A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:47<00:12, 724.25 examples/s]\u001b[A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:47<00:12, 707.93 examples/s]\n",
      "Map:  13%|█▎        | 1000/7764 [00:01<00:07, 902.26 examples/s]s]\u001b[A\n",
      "Map:  48%|████▊     | 21000/43996 [00:28<00:31, 723.64 examples/s]\u001b[A\n",
      "Map:  45%|████▌     | 20000/43996 [00:27<00:31, 750.61 examples/s]\u001b[A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:48<00:11, 720.77 examples/s]\n",
      "Map:  77%|███████▋  | 6000/7764 [00:07<00:02, 739.36 examples/s]s]\u001b[A\n",
      "Map:  26%|██▌       | 2000/7764 [00:02<00:06, 826.02 examples/s]s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [01:01<00:00, 714.53 examples/s]\n",
      "Map:  50%|█████     | 22000/43996 [00:29<00:28, 765.52 examples/s]\n",
      "Map:  48%|████▊     | 21000/43996 [00:28<00:30, 764.18 examples/s]\u001b[A\n",
      "Map:  39%|███▊      | 3000/7764 [00:03<00:06, 792.12 examples/s]\n",
      "Map:  84%|████████▍ | 37000/43996 [00:50<00:10, 636.06 examples/s]A\n",
      "Map:  52%|█████▏    | 23000/43996 [00:30<00:27, 766.01 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:10<00:00, 744.26 examples/s]s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:10<00:00, 753.52 examples/s]\u001b[A\n",
      "Map:  52%|█████▏    | 4000/7764 [00:04<00:04, 788.64 examples/s]\n",
      "Map:  86%|████████▋ | 38000/43996 [00:52<00:08, 686.86 examples/s]A\n",
      "Map:  26%|██▌       | 2000/7764 [00:01<00:05, 1087.63 examples/s]\n",
      "Map:  55%|█████▍    | 24000/43996 [00:32<00:26, 745.10 examples/s]\u001b[A\n",
      "Map:  52%|█████▏    | 23000/43996 [00:31<00:27, 752.81 examples/s]\u001b[A\n",
      "Map:  64%|██████▍   | 5000/7764 [00:06<00:03, 836.97 examples/s]]\u001b[A\n",
      "Map:  39%|███▊      | 3000/7764 [00:03<00:05, 818.00 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:53<00:07, 677.75 examples/s]\u001b[A\n",
      "Map:  57%|█████▋    | 25000/43996 [00:33<00:24, 778.83 examples/s][A\n",
      "Map:  77%|███████▋  | 6000/7764 [00:07<00:02, 813.41 examples/s]s]\u001b[A\n",
      "Map:  64%|██████▍   | 5000/7764 [00:04<00:02, 1329.25 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:54<00:05, 690.82 examples/s]A\n",
      "Map:  59%|█████▉    | 26000/43996 [00:34<00:23, 767.73 examples/s]\u001b[A\n",
      "\n",
      "Map:  90%|█████████ | 7000/7764 [00:08<00:01, 756.50 examples/s]]]\u001b[A\u001b[A\n",
      "Map:  64%|██████▍   | 5000/7764 [00:06<00:03, 742.82 examples/s]\u001b[A\n",
      "Map:  90%|█████████ | 7000/7764 [00:05<00:00, 1273.96 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:56<00:04, 705.90 examples/s]\u001b[A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:35<00:22, 757.74 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:06<00:00, 1223.29 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:10<00:00, 765.14 examples/s]\n",
      "\n",
      "Map:  77%|███████▋  | 6000/7764 [00:07<00:02, 777.48 examples/s]\u001b[A\n",
      "Map:  95%|█████████▌| 42000/43996 [00:57<00:02, 726.23 examples/s]\u001b[A\n",
      "Map:  64%|██████▎   | 28000/43996 [00:37<00:22, 699.29 examples/s]A\n",
      "Map:  61%|██████▏   | 27000/43996 [00:36<00:23, 709.68 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 7764/7764 [00:08<00:00, 865.90 examples/s] \u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:58<00:01, 734.75 examples/s]\n",
      "Map:  66%|██████▌   | 29000/43996 [00:38<00:20, 743.35 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [01:00<00:00, 755.58 examples/s]\u001b[A\n",
      "Map:  68%|██████▊   | 30000/43996 [00:39<00:18, 772.90 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [01:00<00:00, 726.32 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [01:00<00:00, 728.51 examples/s]\n",
      "Map:  70%|███████   | 31000/43996 [00:41<00:16, 786.80 examples/s]\n",
      "Map:  73%|███████▎  | 32000/43996 [00:42<00:15, 792.48 examples/s]\u001b[A\n",
      "Map:  75%|███████▌  | 33000/43996 [00:43<00:13, 797.88 examples/s]\u001b[A\n",
      "Map:  77%|███████▋  | 34000/43996 [00:44<00:12, 799.42 examples/s]\u001b[A\n",
      "Map:  80%|███████▉  | 35000/43996 [00:46<00:10, 828.31 examples/s]\u001b[A\n",
      "Map:  82%|████████▏ | 36000/43996 [00:47<00:10, 792.40 examples/s]\u001b[A\n",
      "Map:  84%|████████▍ | 37000/43996 [00:48<00:08, 813.34 examples/s]\u001b[A\n",
      "Map:  86%|████████▋ | 38000/43996 [00:49<00:07, 830.90 examples/s]\u001b[A\n",
      "Map:  89%|████████▊ | 39000/43996 [00:50<00:06, 823.65 examples/s]\u001b[A\n",
      "Map:  91%|█████████ | 40000/43996 [00:52<00:05, 760.63 examples/s]\u001b[A\n",
      "Map:  93%|█████████▎| 41000/43996 [00:53<00:03, 777.51 examples/s]\u001b[A\n",
      "Map:  95%|█████████▌| 42000/43996 [00:54<00:02, 779.18 examples/s]\u001b[A\n",
      "Map:  98%|█████████▊| 43000/43996 [00:56<00:01, 790.00 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:57<00:00, 828.03 examples/s]\u001b[A\n",
      "Map: 100%|██████████| 43996/43996 [00:57<00:00, 760.52 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 train begin 00:16:14"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 43996/43996 [00:57<00:00, 763.37 examples/s]\u001b[A\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: 12.48\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 224.00M. That was not possible. There are 181.39M free.; (0x0x0_HBM0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 246, in _process_worker\n    r = call_item.fn(*call_item.args, **call_item.kwargs)\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in _process_chunk\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/concurrent/futures/process.py\", line 205, in <listcomp>\n    return [fn(*args) for args in chunk]\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py\", line 83, in wrapper\n    return fn(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 75, in _run_thread_per_device\n    replica_results = list(\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 621, in result_iterator\n    yield _result_or_cancel(fs.pop())\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 319, in _result_or_cancel\n    return fut.result(timeout)\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 458, in result\n    return self.__get_result()\n  File \"/usr/local/lib/python3.10/concurrent/futures/_base.py\", line 403, in __get_result\n    raise self._exception\n  File \"/usr/local/lib/python3.10/concurrent/futures/thread.py\", line 58, in run\n    result = self.fn(*self.args, **self.kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 68, in _thread_fn\n    return fn()\n  File \"/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py\", line 178, in __call__\n    self.fn(runtime.global_ordinal(), *self.args, **self.kwargs)\n  File \"/tmp/ipykernel_51634/1143278732.py\", line 34, in train\n    xm.master_print(f'Loss: {loss:.2f}')\n  File \"/usr/local/lib/python3.10/site-packages/torch/_tensor.py\", line 933, in __format__\n    return self.item().__format__(format_spec)\nRuntimeError: Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 224.00M. That was not possible. There are 181.39M free.; (0x0x0_HBM0)\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mxmp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfork\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:83\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     81\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/distributed/xla_multiprocessing.py:38\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, args, nprocs, join, daemon, start_method)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;129m@xr\u001b[39m\u001b[38;5;241m.\u001b[39mrequires_pjrt\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mspawn\u001b[39m(fn,\n\u001b[1;32m      8\u001b[0m           args\u001b[38;5;241m=\u001b[39m(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     11\u001b[0m           daemon\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m     12\u001b[0m           start_method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspawn\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     13\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Enables multi processing based replication.\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;124;03m    return None.\u001b[39;00m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[0;32m---> 38\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpjrt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mspawn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnprocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:202\u001b[0m, in \u001b[0;36mspawn\u001b[0;34m(fn, nprocs, start_method, args)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m nprocs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m   logging\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUnsupported nprocs (\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m), ignoring...\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m nprocs)\n\u001b[0;32m--> 202\u001b[0m \u001b[43mrun_multiprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mspawn_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstart_method\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstart_method\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/runtime.py:83\u001b[0m, in \u001b[0;36mrequires_pjrt.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m using_pjrt():\n\u001b[1;32m     80\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m` not implemented for XRT\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m     81\u001b[0m       fn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[0;32m---> 83\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:159\u001b[0m, in \u001b[0;36mrun_multiprocess\u001b[0;34m(fn, start_method, *args, **kwargs)\u001b[0m\n\u001b[1;32m    153\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    154\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    155\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    156\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    157\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    158\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[0;32m--> 159\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[43m      \u001b[49m\u001b[43mitertools\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_iterable\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[43m          \u001b[49m\u001b[43mresult\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mprocess_results\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m runtime\u001b[38;5;241m.\u001b[39mdevice_type() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    164\u001b[0m   gpu\u001b[38;5;241m.\u001b[39mshutdown_distributed_runtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/torch_xla/_internal/pjrt.py:160\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    153\u001b[0m   mp_fn \u001b[38;5;241m=\u001b[39m functools\u001b[38;5;241m.\u001b[39mpartial(\n\u001b[1;32m    154\u001b[0m       _run_thread_per_device,\n\u001b[1;32m    155\u001b[0m       local_world_size\u001b[38;5;241m=\u001b[39mnum_processes,\n\u001b[1;32m    156\u001b[0m       fn\u001b[38;5;241m=\u001b[39mfunctools\u001b[38;5;241m.\u001b[39mpartial(fn, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs),\n\u001b[1;32m    157\u001b[0m       initializer_fn\u001b[38;5;241m=\u001b[39minitialize_multiprocess)\n\u001b[1;32m    158\u001b[0m   process_results \u001b[38;5;241m=\u001b[39m executor\u001b[38;5;241m.\u001b[39mmap(mp_fn, \u001b[38;5;28mrange\u001b[39m(num_processes))\n\u001b[1;32m    159\u001b[0m   replica_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(\n\u001b[0;32m--> 160\u001b[0m       itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(\n\u001b[1;32m    161\u001b[0m           result\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m process_results))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m runtime\u001b[38;5;241m.\u001b[39mdevice_type() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGPU\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    164\u001b[0m   gpu\u001b[38;5;241m.\u001b[39mshutdown_distributed_runtime()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/process.py:575\u001b[0m, in \u001b[0;36m_chain_from_iterable_of_lists\u001b[0;34m(iterable)\u001b[0m\n\u001b[1;32m    569\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_chain_from_iterable_of_lists\u001b[39m(iterable):\n\u001b[1;32m    570\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    571\u001b[0m \u001b[38;5;124;03m    Specialized implementation of itertools.chain.from_iterable.\u001b[39;00m\n\u001b[1;32m    572\u001b[0m \u001b[38;5;124;03m    Each item in *iterable* should be a list.  This function is\u001b[39;00m\n\u001b[1;32m    573\u001b[0m \u001b[38;5;124;03m    careful not to keep references to yielded objects.\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 575\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m element \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m    576\u001b[0m         element\u001b[38;5;241m.\u001b[39mreverse()\n\u001b[1;32m    577\u001b[0m         \u001b[38;5;28;01mwhile\u001b[39;00m element:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:621\u001b[0m, in \u001b[0;36mExecutor.map.<locals>.result_iterator\u001b[0;34m()\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m fs:\n\u001b[1;32m    619\u001b[0m     \u001b[38;5;66;03m# Careful not to keep a reference to the popped future\u001b[39;00m\n\u001b[1;32m    620\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 621\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[43m_result_or_cancel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    623\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m _result_or_cancel(fs\u001b[38;5;241m.\u001b[39mpop(), end_time \u001b[38;5;241m-\u001b[39m time\u001b[38;5;241m.\u001b[39mmonotonic())\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:319\u001b[0m, in \u001b[0;36m_result_or_cancel\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    318\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfut\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    321\u001b[0m         fut\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[1;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[0;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/concurrent/futures/_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[1;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[1;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Bad StatusOr access: RESOURCE_EXHAUSTED: Error allocating device buffer: Attempting to allocate 224.00M. That was not possible. There are 181.39M free.; (0x0x0_HBM0)"
     ]
    }
   ],
   "source": [
    "xmp.spawn(train, start_method=\"fork\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 388.410448,
     "end_time": "2023-11-04T13:21:54.038795",
     "exception": false,
     "start_time": "2023-11-04T13:15:25.628347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# model = model.cpu()\n",
    "# print('now saving the model')\n",
    "# model.push_to_hub(\n",
    "#     \"felarof01/llama3-test\", \n",
    "#     tokenizer=tokenizer,\n",
    "#     private=False,\n",
    "#     create_pr=False,\n",
    "#     max_shard_size=\"2GB\", # Sharding isn't as important as before since hardware is better now but who cares anyway\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3555678,
     "isSourceIdPinned": true,
     "sourceId": 6196932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3936750,
     "sourceId": 6847931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946973,
     "sourceId": 6867914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937441,
     "sourceId": 6868189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949797,
     "sourceId": 6873567,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3944051,
     "sourceId": 7060310,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30529,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
