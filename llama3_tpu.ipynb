{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 121.512757,
     "end_time": "2023-11-04T12:34:07.401258",
     "exception": false,
     "start_time": "2023-11-04T12:32:05.888501",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: torch-xla 2.1.0 does not provide the extra 'tpu'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.0.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.1.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers datasets sentencepiece peft -q\n",
    "!pip3 install torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n",
    "!pip3 install torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n",
    "!pip3 install scikit-learn -q\n",
    "!pip3 install huggingface_hub -q\n",
    "!pip3 uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "papermill": {
     "duration": 13.608021,
     "end_time": "2023-11-04T12:34:21.013846",
     "exception": false,
     "start_time": "2023-11-04T12:34:07.405825",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "WARNING:root:PJRT is now the default runtime. For more information, see https://github.com/pytorch/xla/blob/master/docs/pjrt.md\n",
      "WARNING:root:libtpu.so and TPU device found. Setting PJRT_DEVICE=TPU.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datasets\n",
    "import torch.optim as optim\n",
    "import torch_xla.debug.profiler as xp\n",
    "import torch_xla.core.xla_model as xm\n",
    "import torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\n",
    "import torch_xla.distributed.parallel_loader as pl\n",
    "import torch_xla.test.test_utils as test_utils\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import re\n",
    "import torch_xla.experimental.xla_sharding as xs\n",
    "import torch_xla.core.xla_model as xm\n",
    "from transformers import (\n",
    "    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n",
    ") # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n",
    "\n",
    "from transformers import logging as hf_logging\n",
    "import torch.nn.functional as F\n",
    "import torch_xla.runtime as xr\n",
    "\n",
    "xr.use_spmd()\n",
    "\n",
    "import torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\n",
    "from torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\n",
    "from torch_xla.experimental.xla_sharding import Mesh\n",
    "\n",
    "from peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "from dataclasses import dataclass\n",
    "from tqdm import tqdm\n",
    "\n",
    "import transformers\n",
    "import datasets\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datasets import Dataset\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "!export USE_TORCH=True # If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n",
    "!export XLA_USE_BF16=1\n",
    "!export TOKENIZERS_PARALLELISM=false\n",
    "os.environ[\"PJRT_DEVICE\"] = \"TPU\"\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\n",
    "os.environ[\"XLA_USE_BF16\"] = \"1\"\n",
    "\n",
    "hf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\n",
    "MAX_INPUT=512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import importlib\n",
    "\n",
    "if '.' not in sys.path:\n",
    "    sys.path.append('.')\n",
    "\n",
    "spmd_util = importlib.import_module('utils.spmd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /root/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "\n",
    "login(token=\"YOUR_TOKEN\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xla:0\n",
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1721075800.964997   14249 tpu_initializer_framework_helper.cc:78] Libtpu path is: /usr/local/lib/python3.10/site-packages/torch_xla/lib/libtpu.so\n",
      "I0000 00:00:1721075803.516936   14249 pjrt_c_api_client.cc:110] PjRtCApiClient created.\n"
     ]
    }
   ],
   "source": [
    "def test_device_count():\n",
    "    print(xm.xla_device())\n",
    "    print(xr.global_runtime_device_count())\n",
    "test_device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"meta-llama/Meta-Llama-3-8B\"\n",
    "MODEL = MODEL_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "papermill": {
     "duration": 0.547357,
     "end_time": "2023-11-04T12:34:26.034497",
     "exception": false,
     "start_time": "2023-11-04T12:34:25.48714",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading shards: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [02:44<00:00, 41.05s/it]\n",
      "Loading checkpoint shards: 100%|███████████████████████████████████████████████████████████████████████████████████████████████| 4/4 [00:01<00:00,  2.07it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "model = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2, torch_dtype=torch.bfloat16)\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "papermill": {
     "duration": 0.027467,
     "end_time": "2023-11-04T12:40:23.239228",
     "exception": false,
     "start_time": "2023-11-04T12:40:23.211761",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FLAGS = {'MAX_INPUT': 512,\n",
    "         'LOGGING_STEPS': 100,\n",
    "         'NUM_EPOCHS': 2,\n",
    "         'BATCH_SIZE': 1, # Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n",
    "          'NUM_STEPS': 8} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|███████████████████████████████████████████████████████████████████████████████████████████████████| 51760/51760 [00:01<00:00, 26165.70 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:00<00:00, 1051.95 examples/s]\n",
      "Map: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████| 512/512 [00:00<00:00, 1152.46 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset prepared and preprocessed\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Alpaca prompt template\n",
    "alpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
    "\n",
    "### Instruction: {}\n",
    "\n",
    "### Input: {}\n",
    "\n",
    "### Response: {}\"\"\"\n",
    "\n",
    "EOS_TOKEN = tokenizer.eos_token\n",
    "\n",
    "def formatting_prompts_func(examples):\n",
    "    instructions = examples[\"instruction\"]\n",
    "    inputs = examples[\"input\"]\n",
    "    outputs = examples[\"output\"]\n",
    "    texts = []\n",
    "    for instruction, input, output in zip(instructions, inputs, outputs):\n",
    "        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n",
    "        texts.append(text)\n",
    "    return {\"text\": texts}\n",
    "\n",
    "# Load and format the dataset\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "# Create dummy label.\n",
    "dataset = dataset.add_column(\"label\", [1] * len(dataset))\n",
    "def preprocess_function(examples):\n",
    "    tokenized = tokenizer(examples['text'], max_length=512, padding='max_length', truncation=True)\n",
    "    tokenized['label'] = examples['label']\n",
    "    return tokenized\n",
    "\n",
    "# Limiting to 512 rows in data in demo example.\n",
    "ds = dataset.train_test_split(test_size=0.15)\n",
    "ds['train'] = ds['train'].select(range(min(512, len(ds['train'])))).map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "ds['test'] = ds['test'].select(range(min(512, len(ds['test'])))).map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n",
    "\n",
    "training_loader = torch.utils.data.DataLoader(ds['train'], batch_size=FLAGS['BATCH_SIZE'], collate_fn=DataCollatorWithPadding(tokenizer=tokenizer))\n",
    "testing_loader = torch.utils.data.DataLoader(ds['test'], batch_size=FLAGS['BATCH_SIZE'], collate_fn=DataCollatorWithPadding(tokenizer=tokenizer))\n",
    "print('Dataset prepared and preprocessed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader:\n",
      "input_ids: shape torch.Size([1, 512]), dtype torch.int64\n",
      "attention_mask: shape torch.Size([1, 512]), dtype torch.int64\n",
      "labels: shape torch.Size([1]), dtype torch.int64\n"
     ]
    }
   ],
   "source": [
    "# Test the DataLoader\n",
    "print(\"Testing DataLoader:\")\n",
    "batch = next(iter(training_loader))\n",
    "for k, v in batch.items():\n",
    "    if isinstance(v, torch.Tensor):\n",
    "        print(f\"{k}: shape {v.shape}, dtype {v.dtype}\")\n",
    "    else:\n",
    "        print(f\"{k}: {type(v)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "papermill": {
     "duration": 29.992732,
     "end_time": "2023-11-04T12:40:53.298154",
     "exception": false,
     "start_time": "2023-11-04T12:40:23.305422",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Only train a few params in demo code.\n",
    "# TODO: switch to LoRA.\n",
    "cnt = 0\n",
    "for param in model.parameters():\n",
    "    cnt += 1\n",
    "    param.requires_grad = False\n",
    "    if cnt > 270:\n",
    "        param.requires_grad = True\n",
    "\n",
    "config = AutoConfig.from_pretrained(MODEL)\n",
    "config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "num_devices = xr.global_runtime_device_count()\n",
    "mesh_shape = (1, num_devices, 1)\n",
    "device_ids = np.array(range(num_devices))\n",
    "mesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\n",
    "spmd_util.partition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "papermill": {
     "duration": 1.912133,
     "end_time": "2023-11-04T12:40:55.230144",
     "exception": false,
     "start_time": "2023-11-04T12:40:53.318011",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(FLAGS):\n",
    "    device=xm.xla_device()\n",
    "    num_iterations = int(FLAGS['NUM_STEPS'] / FLAGS['BATCH_SIZE'])\n",
    "    lr = 1e-5\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE'])\n",
    "\n",
    "    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n",
    "        model.train()\n",
    "        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n",
    "        \n",
    "        for step, batch in enumerate(training_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Process each item in the batch separately\n",
    "            batch_loss = 0\n",
    "            for i in range(len(batch.input_ids)):\n",
    "                input_ids = batch.input_ids[i].unsqueeze(0).to(device)\n",
    "                attention_mask = batch.attention_mask[i].unsqueeze(0).to(device)\n",
    "                labels = batch.labels[i].unsqueeze(0).unsqueeze(0).to(device)\n",
    "                \n",
    "                xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "                xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "                xs.mark_sharding(labels, mesh, (0, 1))\n",
    "                \n",
    "                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                loss = outputs.loss\n",
    "                batch_loss += loss\n",
    "            \n",
    "            # Average the loss over the batch\n",
    "            batch_loss /= len(batch.input_ids)\n",
    "            batch_loss.backward()\n",
    "            optimizer.step()\n",
    "            xm.mark_step()\n",
    "            \n",
    "            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n",
    "                print(f'loss: {batch_loss.item()}, time: {test_utils.now()}, step: {step}')\n",
    "            \n",
    "            scheduler.step()\n",
    "        \n",
    "        model.eval()\n",
    "        total_loss = 0.0\n",
    "        total_steps = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for step, batch in enumerate(testing_loader):\n",
    "                batch_loss = 0\n",
    "                for i in range(len(batch.input_ids)):\n",
    "                    input_ids = batch.input_ids[i].unsqueeze(0).to(device)\n",
    "                    attention_mask = batch.attention_mask[i].unsqueeze(0).to(device)\n",
    "                    labels = batch.labels[i].unsqueeze(0).unsqueeze(0).to(device)\n",
    "                    \n",
    "                    xs.mark_sharding(input_ids, mesh, (0, 1))\n",
    "                    xs.mark_sharding(attention_mask, mesh, (0, 1))\n",
    "                    xs.mark_sharding(labels, mesh, (0, 1))\n",
    "                    \n",
    "                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n",
    "                    loss = outputs.loss\n",
    "                    batch_loss += loss.item()\n",
    "                \n",
    "                total_loss += batch_loss / len(batch.input_ids)\n",
    "                total_steps += 1\n",
    "        \n",
    "        average_loss = total_loss / total_steps\n",
    "        xm.master_print('Epoch {} test end {}, test loss={:.2f}'.format(epoch, test_utils.now(), average_loss))\n",
    "        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 2070.358248,
     "end_time": "2023-11-04T13:15:25.607979",
     "exception": false,
     "start_time": "2023-11-04T12:40:55.249731",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(FLAGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 388.410448,
     "end_time": "2023-11-04T13:21:54.038795",
     "exception": false,
     "start_time": "2023-11-04T13:15:25.628347",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = model.cpu()\n",
    "print('now saving the model')\n",
    "model.push_to_hub(\n",
    "    \"felarof01/llama3-test\", \n",
    "    tokenizer=tokenizer,\n",
    "    private=False,\n",
    "    create_pr=False,\n",
    "    max_shard_size=\"2GB\", # Sharding isn't as important as before since hardware is better now but who cares anyway\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "tpu1vmV38",
   "dataSources": [
    {
     "databundleVersionId": 7516023,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 3555678,
     "isSourceIdPinned": true,
     "sourceId": 6196932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3863727,
     "sourceId": 6703755,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3936750,
     "sourceId": 6847931,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3946973,
     "sourceId": 6867914,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937441,
     "sourceId": 6868189,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3949797,
     "sourceId": 6873567,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3942644,
     "sourceId": 6890527,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3937250,
     "sourceId": 7017419,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3944051,
     "sourceId": 7060310,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30529,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
