{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.8.17","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpu1vmV38","dataSources":[{"sourceId":61542,"databundleVersionId":7516023,"sourceType":"competition"},{"sourceId":6196932,"sourceType":"datasetVersion","datasetId":3555678,"isSourceIdPinned":true},{"sourceId":6703755,"sourceType":"datasetVersion","datasetId":3863727},{"sourceId":6847931,"sourceType":"datasetVersion","datasetId":3936750},{"sourceId":6867914,"sourceType":"datasetVersion","datasetId":3946973},{"sourceId":6868189,"sourceType":"datasetVersion","datasetId":3937441},{"sourceId":6873567,"sourceType":"datasetVersion","datasetId":3949797},{"sourceId":6890527,"sourceType":"datasetVersion","datasetId":3942644},{"sourceId":7017419,"sourceType":"datasetVersion","datasetId":3937250},{"sourceId":7060310,"sourceType":"datasetVersion","datasetId":3944051}],"dockerImageVersionId":30529,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip3 install transformers datasets sentencepiece peft -q\n!pip install torch~=2.1.0 --index-url https://download.pytorch.org/whl/cpu -q # Updating torch since we need the latest version\n!pip install torch_xla[tpu]~=2.1.0 -f https://storage.googleapis.com/libtpu-releases/index.html -q\n!pip uninstall tensorflow -y # If we don't do this, TF will take over TPU and cause permission error for PT\n!cp /kaggle/input/utils-xla/spmd_util.py . # From this repo: https://github.com/HeegyuKim/torch-xla-SPMD","metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","papermill":{"duration":121.512757,"end_time":"2023-11-04T12:34:07.401258","exception":false,"start_time":"2023-11-04T12:32:05.888501","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"!pip3 install scikit-learn -q\n!pip3 install huggingface_hub -q","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import os\nimport pandas as pd\nimport numpy as np\nimport datasets\nimport torch.optim as optim\nimport torch_xla.debug.profiler as xp\nimport torch_xla.core.xla_model as xm\nimport torch_xla.distributed.xla_multiprocessing as xmp # We also import mp modules if we wanna use that for some reason\nimport torch_xla.distributed.parallel_loader as pl\nimport torch_xla.test.test_utils as test_utils\nimport torch\nimport torch.nn as nn\nimport re\nimport torch_xla.experimental.xla_sharding as xs\nimport torch_xla.core.xla_model as xm\nfrom transformers import (\n    GPTNeoXConfig, T5Config, LlamaConfig, AutoTokenizer, AutoModelForCausalLM, DataCollatorWithPadding, AutoConfig, AutoModelForSequenceClassification\n) # You can use any of models with those configs (even flan T5 xxl!). Other models are not supported.\n\nfrom transformers import logging as hf_logging\nimport torch.nn.functional as F\nimport torch_xla.runtime as xr\n\nxr.use_spmd()\n\nimport torch_xla.experimental.xla_sharding as xs # \"experimental\" prefix always means you're gonna have a good time LMAO\nfrom torch_xla.experimental.xla_sharded_tensor import XLAShardedTensor\nfrom torch_xla.experimental.xla_sharding import Mesh\n\nfrom peft import LoraConfig, TaskType, get_peft_model # If we wanna use peft. Quantazation requiers GPU though. You'll have to download already quantazed models\nfrom spmd_util import partition_module                # You could experiment with using already quantazed models like 4bit/Llama-2-7b-Chat-GPTQ if you're feeling funny\nfrom datasets import Dataset, load_dataset, concatenate_datasets\nfrom dataclasses import dataclass\nfrom tqdm import tqdm\n\nimport transformers\nimport datasets\nimport pandas as pd\nimport numpy as np\nfrom datasets import Dataset\nfrom sklearn.metrics import roc_auc_score\n\n!export USE_TORCH=True # If we don't do this, transformers will seemingly bork the session upon import. Really weird error.\n!export XLA_USE_BF16=1\n!export TOKENIZERS_PARALLELISM=false\nos.environ[\"PJRT_DEVICE\"] = \"TPU\"\nos.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"\nos.environ[\"XLA_USE_BF16\"] = \"1\"\n\nhf_logging.set_verbosity_error() # It can still display warnings which is a bit annoying but whatever\nMAX_INPUT=512","metadata":{"papermill":{"duration":13.608021,"end_time":"2023-11-04T12:34:21.013846","exception":false,"start_time":"2023-11-04T12:34:07.405825","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import login\n\nlogin(token=\"YOUR_TOKEN\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def test_device_count():\n    print(xm.xla_device())\n    print(xr.global_runtime_device_count())\ntest_device_count()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"MODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\nMODEL = MODEL_NAME","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoTokenizer\nmodel = AutoModelForSequenceClassification.from_pretrained(MODEL, num_labels=2, torch_dtype=torch.bfloat16)\ntokenizer = AutoTokenizer.from_pretrained(MODEL)\ntokenizer.pad_token = tokenizer.eos_token","metadata":{"papermill":{"duration":0.547357,"end_time":"2023-11-04T12:34:26.034497","exception":false,"start_time":"2023-11-04T12:34:25.48714","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"FLAGS = {'MAX_INPUT': 512,\n         'LOGGING_STEPS': 100,\n         'NUM_EPOCHS': 2,\n         'BATCH_SIZE': 1, # Making batch_size lower then 8 will result in slower training, but will allow for larger models\\context. Fortunately, we have 128GBs. Setting higher batch_size doesn't seem to improve time.\n          'NUM_STEPS': 8} ","metadata":{"papermill":{"duration":0.027467,"end_time":"2023-11-04T12:40:23.239228","exception":false,"start_time":"2023-11-04T12:40:23.211761","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\n# Alpaca prompt template\nalpaca_prompt = \"\"\"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction: {}\n\n### Input: {}\n\n### Response: {}\"\"\"\n\nEOS_TOKEN = tokenizer.eos_token\n\ndef formatting_prompts_func(examples):\n    instructions = examples[\"instruction\"]\n    inputs = examples[\"input\"]\n    outputs = examples[\"output\"]\n    texts = []\n    for instruction, input, output in zip(instructions, inputs, outputs):\n        text = alpaca_prompt.format(instruction, input, output) + EOS_TOKEN\n        texts.append(text)\n    return {\"text\": texts}\n\n# Load and format the dataset\ndataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train\")\ndataset = dataset.map(formatting_prompts_func, batched=True)\n\n# Create labels (assuming all examples are AI-generated)\ndataset = dataset.add_column(\"label\", [1] * len(dataset))\n\n# Define preprocessing function\ndef preprocess_function(examples):\n    tokenized = tokenizer(examples['text'], max_length=512, padding='max_length', truncation=True)\n    tokenized['label'] = examples['label']\n    return tokenized\n\n# Split the dataset\nds = dataset.train_test_split(test_size=0.15)\n\n# Apply preprocessing\nds['train'] = ds['train'].select(range(min(512, len(ds['train'])))).map(preprocess_function, batched=True, remove_columns=dataset.column_names)\nds['test'] = ds['test'].select(range(min(512, len(ds['test'])))).map(preprocess_function, batched=True, remove_columns=dataset.column_names)\n\nprint('Dataset prepared and preprocessed')\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)\ntraining_loader = torch.utils.data.DataLoader(ds['train'], batch_size=FLAGS['BATCH_SIZE'], collate_fn=data_collator)\ntesting_loader = torch.utils.data.DataLoader(ds['test'], batch_size=FLAGS['BATCH_SIZE'], collate_fn=data_collator)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test the DataLoader\nprint(\"Testing DataLoader:\")\nbatch = next(iter(training_loader))\nfor k, v in batch.items():\n    if isinstance(v, torch.Tensor):\n        print(f\"{k}: shape {v.shape}, dtype {v.dtype}\")\n    else:\n        print(f\"{k}: {type(v)}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cnt = 0\nfor param in model.parameters():\n    cnt += 1\n    param.requires_grad = False\n    if cnt > 270:\n        param.requires_grad = True\n\nconfig = AutoConfig.from_pretrained(MODEL)\nconfig.pad_token_id = tokenizer.pad_token_id\n\nnum_devices = xr.global_runtime_device_count()\nmesh_shape = (1, num_devices, 1)\ndevice_ids = np.array(range(num_devices))\nmesh = Mesh(device_ids, mesh_shape, ('dp', 'fsdp', 'mp'))\npartition_module(model, mesh) # After this, the model is sharded between cores but still has the same API as if it was on single device. Neat.","metadata":{"papermill":{"duration":29.992732,"end_time":"2023-11-04T12:40:53.298154","exception":false,"start_time":"2023-11-04T12:40:23.305422","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def train(FLAGS):\n    num_iterations = int(FLAGS['NUM_STEPS'] / FLAGS['BATCH_SIZE'])\n    lr = 1e-5\n    optimizer = optim.Adam(model.parameters(), lr=lr)\n    scheduler = torch.optim.lr_scheduler.LinearLR(optimizer, start_factor=1.0, end_factor=0.1, total_iters=FLAGS['NUM_STEPS'] * FLAGS['BATCH_SIZE'])\n\n    for epoch in range(1, FLAGS['NUM_EPOCHS'] + 1):\n        model.train()\n        xm.master_print('Epoch {} train begin {}'.format(epoch, test_utils.now()))\n        \n        for step, batch in enumerate(training_loader):\n            optimizer.zero_grad()\n            \n            # Process each item in the batch separately\n            batch_loss = 0\n            for i in range(len(batch.input_ids)):\n                input_ids = batch.input_ids[i].unsqueeze(0).to(device)\n                attention_mask = batch.attention_mask[i].unsqueeze(0).to(device)\n                labels = batch.labels[i].unsqueeze(0).unsqueeze(0).to(device)\n                \n                xs.mark_sharding(input_ids, mesh, (0, 1))\n                xs.mark_sharding(attention_mask, mesh, (0, 1))\n                xs.mark_sharding(labels, mesh, (0, 1))\n                \n                outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                loss = outputs.loss\n                batch_loss += loss\n            \n            # Average the loss over the batch\n            batch_loss /= len(batch.input_ids)\n            batch_loss.backward()\n            optimizer.step()\n            xm.mark_step()\n            \n            if (step + 1) % FLAGS['LOGGING_STEPS'] == 0:\n                print(f'loss: {batch_loss.item()}, time: {test_utils.now()}, step: {step}')\n            \n            scheduler.step()\n        \n        model.eval()\n        total_loss = 0.0\n        total_steps = 0\n        \n        with torch.no_grad():\n            for step, batch in enumerate(testing_loader):\n                batch_loss = 0\n                for i in range(len(batch.input_ids)):\n                    input_ids = batch.input_ids[i].unsqueeze(0).to(device)\n                    attention_mask = batch.attention_mask[i].unsqueeze(0).to(device)\n                    labels = batch.labels[i].unsqueeze(0).unsqueeze(0).to(device)\n                    \n                    xs.mark_sharding(input_ids, mesh, (0, 1))\n                    xs.mark_sharding(attention_mask, mesh, (0, 1))\n                    xs.mark_sharding(labels, mesh, (0, 1))\n                    \n                    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)\n                    loss = outputs.loss\n                    batch_loss += loss.item()\n                \n                total_loss += batch_loss / len(batch.input_ids)\n                total_steps += 1\n        \n        average_loss = total_loss / total_steps\n        xm.master_print('Epoch {} test end {}, test loss={:.2f}'.format(epoch, test_utils.now(), average_loss))\n        xm.master_print('Epoch {} train end {}'.format(epoch, test_utils.now()))","metadata":{"papermill":{"duration":1.912133,"end_time":"2023-11-04T12:40:55.230144","exception":false,"start_time":"2023-11-04T12:40:53.318011","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train(FLAGS)","metadata":{"papermill":{"duration":2070.358248,"end_time":"2023-11-04T13:15:25.607979","exception":false,"start_time":"2023-11-04T12:40:55.249731","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = model.cpu()\nprint('now saving the model')\nmodel.push_to_hub(\n    \"felarof01/llama2\", \n    tokenizer=tokenizer,\n    private=False,\n    create_pr=False,\n    max_shard_size=\"2GB\", # Sharding isn't as important as before since hardware is better now but who cares anyway\n)","metadata":{"papermill":{"duration":388.410448,"end_time":"2023-11-04T13:21:54.038795","exception":false,"start_time":"2023-11-04T13:15:25.628347","status":"completed"},"tags":[],"trusted":true},"execution_count":null,"outputs":[]}]}