{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install tensorflow-cpu -q\n",
    "!pip install tensorflow_datasets -q\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "# For LoRA\n",
    "import lorax\n",
    "\n",
    "# We will use HuggingFace's dataset, tokenizer, and model classes.\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "INPUT: Please provide your HUGGINGFACE_USERNAME:  felarof01\n",
      "INPUT: Please provide your HUGGINGFACE_TOKEN:  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\n"
     ]
    }
   ],
   "source": [
    "# HuggingFace username and token to use when downloading.\n",
    "MODEL_NAME=\"felafax/gemma-2-2b-it-Flax\"\n",
    "HUGGINGFACE_USERNAME = input(\"INPUT: Please provide your HUGGINGFACE_USERNAME: \")\n",
    "HUGGINGFACE_TOKEN = input(\"INPUT: Please provide your HUGGINGFACE_TOKEN: \")\n",
    "\n",
    "model_name=MODEL_NAME\n",
    "hugging_face_token=HUGGINGFACE_TOKEN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "from huggingface_hub import snapshot_download\n",
    "\n",
    "ckpt_path = snapshot_download(repo_id=MODEL_NAME, token=HUGGINGFACE_TOKEN)\n",
    "vocab_path = os.path.join(ckpt_path, 'tokenizer.model')\n",
    "\n",
    "print(ckpt_path)\n",
    "print()\n",
    "print(vocab_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters.\n",
    "params = params_lib.load_and_format_params(os.path.join(ckpt_path, 'gemma2-2b-it'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model config.\n",
    "config = transformer_lib.TransformerConfig.gemma2_2b(cache_size=30)\n",
    "model = transformer_lib.Transformer(config=config)\n",
    "\n",
    "# You can also infer the model config by using the number of layers in the params.\n",
    "# config_2b = transformer_lib.TransformerConfig.from_params(params, cache_size=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for path, param in flat_params.items():\n",
    "        # Join the path components to create a string name\n",
    "        name = \"/\".join(str(x) for x in path)\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"dtype: {param.dtype}\")\n",
    "        # print(f\"Value: {param}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print params before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: transformer/embedder/input_embedding\n",
      "Shape: (256128, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/final_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_0/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_1/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_10/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_11/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_12/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_13/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_14/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_15/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_16/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_17/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_18/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_19/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_2/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_20/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_21/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_22/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_23/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_24/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_25/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_3/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_4/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_5/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_6/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_7/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_8/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/attn/attn_vec_einsum/w\n",
      "Shape: (8, 256, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/attn/kv_einsum/w\n",
      "Shape: (2, 4, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/attn/q_einsum/w\n",
      "Shape: (8, 2304, 256)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/mlp/gating_einsum\n",
      "Shape: (2, 2304, 9216)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/mlp/linear\n",
      "Shape: (9216, 2304)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/post_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/post_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/pre_attention_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n",
      "Name: transformer/layer_9/pre_ffw_norm/scale\n",
      "Shape: (2304,)\n",
      "dtype: bfloat16\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_params(params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### print params after applying LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(hidden_dim=1024, output_dim=2048)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(jax.random.PRNGKey(99), jnp.ones(shape=(256)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: params/Dense_0/kernel\n",
      "Shape: (256, 1024)\n",
      "dtype: float32\n",
      "----------------------------------------\n",
      "Name: params/Dense_0/bias\n",
      "Shape: (1024,)\n",
      "dtype: float32\n",
      "----------------------------------------\n",
      "Name: params/Dense_1/kernel\n",
      "Shape: (1024, 2048)\n",
      "dtype: float32\n",
      "----------------------------------------\n",
      "Name: params/Dense_1/bias\n",
      "Shape: (2048,)\n",
      "dtype: float32\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorax.constants import LORA_FULL, LORA_FREEZE\n",
    "\n",
    "def decision_fn(path, param):\n",
    "    if 'embedding' in path:\n",
    "        print(f'Fully finetuning param {path}')\n",
    "        return LORA_FULL\n",
    "    dim = 9\n",
    "    print(f'Using LoRA with dim={dim} for param {path}')\n",
    "    return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoRA with dim=9 for param (DictKey(key='params'), DictKey(key='Dense_0'), DictKey(key='kernel'))\n",
      "Using LoRA with dim=9 for param (DictKey(key='params'), DictKey(key='Dense_1'), DictKey(key='kernel'))\n"
     ]
    }
   ],
   "source": [
    "lora_spec = lorax.simple_spec(params, decision_fn=decision_fn, tune_vectors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'bias': -1, 'kernel': 9},\n",
       "  'Dense_1': {'bias': -1, 'kernel': 9}}}"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params = lorax.init_lora(params, lora_spec, jax.random.PRNGKey(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'params': {'Dense_0': {'bias': Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       "   'kernel': LoraWeight(shape=(256, 1024), dtype=dtype('float32'), w=Array([[ 0.02465647,  0.11869204, -0.02218285, ...,  0.0543809 ,\n",
       "            0.01860113,  0.02913267],\n",
       "          [-0.04722938, -0.09825108, -0.05490235, ..., -0.07258904,\n",
       "            0.03278206,  0.04242936],\n",
       "          [ 0.03249395,  0.0163221 , -0.1063282 , ...,  0.02276618,\n",
       "            0.01482182,  0.03653511],\n",
       "          ...,\n",
       "          [ 0.04219165,  0.08699494,  0.06926762, ..., -0.00728992,\n",
       "            0.07881317, -0.06749069],\n",
       "          [-0.10112067, -0.03219467, -0.03216508, ...,  0.09708063,\n",
       "           -0.0561055 ,  0.07836296],\n",
       "          [-0.06671629,  0.01709651,  0.01269127, ...,  0.05410413,\n",
       "            0.05015748,  0.00996237]], dtype=float32), a=Array([[ 7.6109534e-03, -1.6037667e-02,  4.7682244e-03, ...,\n",
       "           -2.3265127e-02,  6.5510995e-03,  1.5844539e-02],\n",
       "          [ 8.5565168e-03, -1.5911995e-02, -1.0232066e-03, ...,\n",
       "           -5.3116074e-03,  2.5665363e-02,  1.4823348e-02],\n",
       "          [ 9.7785275e-03, -8.9352485e-03, -6.2589906e-03, ...,\n",
       "            1.5042906e-04,  4.5856177e-03,  8.2078176e-03],\n",
       "          ...,\n",
       "          [ 9.0925517e-03,  1.5331189e-03, -6.4088055e-03, ...,\n",
       "            1.8445466e-02, -1.4564838e-03,  8.1840996e-04],\n",
       "          [ 4.5479783e-03,  6.2977457e-03, -8.1886974e-05, ...,\n",
       "           -1.3668589e-02, -4.9580554e-03,  2.3141250e-03],\n",
       "          [-3.5124058e-03, -1.4075259e-02,  7.1406569e-03, ...,\n",
       "           -1.6282706e-02,  1.1718955e-02,  8.9063225e-03]], dtype=float32), b=Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), alpha=1.0)},\n",
       "  'Dense_1': {'bias': Array([0., 0., 0., ..., 0., 0., 0.], dtype=float32),\n",
       "   'kernel': LoraWeight(shape=(1024, 2048), dtype=dtype('float32'), w=Array([[ 0.02989838,  0.05957387, -0.06001767, ...,  0.02565192,\n",
       "           -0.02730656,  0.04106225],\n",
       "          [-0.0119723 , -0.03158696,  0.02342408, ..., -0.03185182,\n",
       "           -0.02179446, -0.03342338],\n",
       "          [-0.01106615,  0.00842374, -0.04047569, ...,  0.03606943,\n",
       "           -0.02677421, -0.03930258],\n",
       "          ...,\n",
       "          [-0.03888506, -0.03848906, -0.0381063 , ..., -0.02673981,\n",
       "            0.00309442, -0.01466462],\n",
       "          [ 0.01206335,  0.03936036, -0.01058828, ..., -0.07092134,\n",
       "           -0.02912031,  0.00464743],\n",
       "          [-0.02643372, -0.00716437,  0.03526103, ..., -0.05604602,\n",
       "           -0.01033341, -0.01249017]], dtype=float32), a=Array([[ 0.00117138,  0.00029074,  0.00946521, ..., -0.008284  ,\n",
       "           -0.0029979 , -0.01413818],\n",
       "          [ 0.01335018, -0.00331147,  0.00220015, ...,  0.00588008,\n",
       "           -0.00043429,  0.00594487],\n",
       "          [-0.00160571, -0.00133494, -0.02089581, ..., -0.00915385,\n",
       "           -0.00071464,  0.00338997],\n",
       "          ...,\n",
       "          [ 0.00794652,  0.00327183, -0.00266947, ..., -0.02119329,\n",
       "           -0.0156667 ,  0.00855838],\n",
       "          [-0.00893521, -0.00375515, -0.00895489, ..., -0.00491618,\n",
       "           -0.01846189, -0.00076427],\n",
       "          [ 0.00492948, -0.0024947 , -0.01607551, ...,  0.02214688,\n",
       "            0.0181145 ,  0.00070244]], dtype=float32), b=Array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          ...,\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.],\n",
       "          [0., 0., 0., ..., 0., 0., 0.]], dtype=float32), alpha=1.0)}}}"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lorax.lora(model.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.0578917 , -0.76810735,  0.27661797, ..., -0.0036779 ,\n",
       "        0.09208131, -0.6084197 ], dtype=float32)"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model(params, jnp.ones(256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-1.0578917 , -0.76810735,  0.27661797, ..., -0.0036779 ,\n",
       "        0.09208131, -0.6084197 ], dtype=float32)"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.apply(params, jnp.ones(256))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
