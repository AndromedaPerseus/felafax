{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install --upgrade kagglehub -q\n",
    "!pip install ipywidgets -q\n",
    "!pip install tensorflow-cpu -q\n",
    "!pip install tensorflow_datasets -q\n",
    "!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cpu -q\n",
    "!pip install git+https://github.com/felafax/gemma.git -q\n",
    "!pip install qax -q\n",
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install jax-lorax -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['HF_HUB_CACHE'] = '/mnt/persistent-disk/hf/'\n",
    "os.environ['HF_HOME'] = '/mnt/persistent-disk/hf/'\n",
    "!export HF_HUB_CACHE=\"/mnt/persistent-disk/hf/\"\n",
    "!export HF_HOME=\"/mnt/persistent-disk/hf/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "yWaP_LPoEcoY"
   },
   "outputs": [],
   "source": [
    "# @title Python imports\n",
    "\n",
    "import enum\n",
    "import re\n",
    "import string\n",
    "\n",
    "# We import JAX and some related packages.\n",
    "import chex\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "import optax\n",
    "from functools import partial\n",
    "\n",
    "# For LoRA\n",
    "import lorax\n",
    "\n",
    "# We will use HuggingFace's dataset, tokenizer, and model classes.\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer, default_data_collator\n",
    "from datasets import Dataset, load_dataset, concatenate_datasets\n",
    "import torch\n",
    "\n",
    "# Finally, we import Gemma.\n",
    "from gemma import params as params_lib\n",
    "from gemma import sampler as sampler_lib\n",
    "from gemma import transformer as transformer_lib\n",
    "import sentencepiece as spm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0),\n",
       " TpuDevice(id=1, process_index=0, coords=(0,0,0), core_on_chip=1),\n",
       " TpuDevice(id=2, process_index=0, coords=(1,0,0), core_on_chip=0),\n",
       " TpuDevice(id=3, process_index=0, coords=(1,0,0), core_on_chip=1),\n",
       " TpuDevice(id=4, process_index=0, coords=(0,1,0), core_on_chip=0),\n",
       " TpuDevice(id=5, process_index=0, coords=(0,1,0), core_on_chip=1),\n",
       " TpuDevice(id=6, process_index=0, coords=(1,1,0), core_on_chip=0),\n",
       " TpuDevice(id=7, process_index=0, coords=(1,1,0), core_on_chip=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_VsT2o6JEcoZ"
   },
   "source": [
    "## Fine tuning the Gemma model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import flax\n",
    "from flax.traverse_util import flatten_dict\n",
    "\n",
    "def print_params(params):\n",
    "    flat_params = flatten_dict(params)    \n",
    "    for path, param in flat_params.items():\n",
    "        # Join the path components to create a string name\n",
    "        name = \"/\".join(str(x) for x in path)\n",
    "        print(f\"Name: {name}\")\n",
    "        print(f\"Shape: {param.shape}\")\n",
    "        print(f\"dtype: {param.dtype}\")\n",
    "        print(f\"Value: {param}\")\n",
    "        print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try LoRA with simpleNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 1 \n",
    "hidden_dim = 2\n",
    "output_dim = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    hidden_dim: int\n",
    "    output_dim: int\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = nn.Dense(features=self.hidden_dim)(x)\n",
    "        x = nn.relu(x)\n",
    "        x = nn.Dense(features=self.output_dim)(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(hidden_dim=hidden_dim, output_dim=output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = model.init(jax.random.PRNGKey(99), jnp.ones(shape=(1, input_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = params[\"params\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dense_0/kernel\n",
      "Shape: (1, 2)\n",
      "dtype: float32\n",
      "Value: [[-0.17805344  0.61719763]]\n",
      "----------------------------------------\n",
      "Name: Dense_0/bias\n",
      "Shape: (2,)\n",
      "dtype: float32\n",
      "Value: [0. 0.]\n",
      "----------------------------------------\n",
      "Name: Dense_1/kernel\n",
      "Shape: (2, 1)\n",
      "dtype: float32\n",
      "Value: [[ 0.330739  ]\n",
      " [-0.07981248]]\n",
      "----------------------------------------\n",
      "Name: Dense_1/bias\n",
      "Shape: (1,)\n",
      "dtype: float32\n",
      "Value: [0.]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_params(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([-0.04926008], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = model.apply({\"params\": params}, jnp.ones(input_dim))\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.training import train_state  # Useful dataclass to keep train state\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_state.TrainState.create(\n",
    "    apply_fn=model.apply,  # forward pass func\n",
    "    params=params,   # model weights\n",
    "    tx=optax.sgd(learning_rate=0.1)  # optimizer func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, state, batch):\n",
    "  input_images, labels = batch\n",
    "    \n",
    "  # call forward pass function.\n",
    "  logits = state.apply_fn({\"params\": params}, input_images)\n",
    "\n",
    "  # compute loss\n",
    "  loss = optax.squared_error(logits, labels)\n",
    "  loss = loss.mean()\n",
    "  return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(state, batch):\n",
    "  # create a function to compute gradients wrt to loss\n",
    "  # returned by our `forward_pass` function.\n",
    "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0), has_aux=True)\n",
    "\n",
    "  # compute gradients.\n",
    "  (loss, _), grads = grad_fn(state.params, state, batch)\n",
    "  pdb.set_trace()\n",
    "  print(grads)\n",
    "\n",
    "\n",
    "  # apply gradients.\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch):\n",
    "  return backward_pass(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m/tmp/ipykernel_7199/1853935344.py\u001b[0m(9)\u001b[0;36mbackward_pass\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m      7 \u001b[0;31m  \u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m      8 \u001b[0;31m  \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m----> 9 \u001b[0;31m  \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     10 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     11 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  grads\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dense_0': {'bias': Array([0.        , 0.00786314], dtype=float32), 'kernel': Array([[0.        , 0.00786314]], dtype=float32)}, 'Dense_1': {'bias': Array([-0.09852015], dtype=float32), 'kernel': Array([[ 0.       ],\n",
      "       [-0.0608064]], dtype=float32)}}\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ipdb>  c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Dense_0': {'bias': Array([0.        , 0.00786314], dtype=float32), 'kernel': Array([[0.        , 0.00786314]], dtype=float32)}, 'Dense_1': {'bias': Array([-0.09852015], dtype=float32), 'kernel': Array([[ 0.       ],\n",
      "       [-0.0608064]], dtype=float32)}}\n"
     ]
    }
   ],
   "source": [
    "state, loss = train_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dense_0/bias\n",
      "Shape: (2,)\n",
      "dtype: float32\n",
      "Value: [ 0.         -0.00078631]\n",
      "----------------------------------------\n",
      "Name: Dense_0/kernel\n",
      "Shape: (1, 2)\n",
      "dtype: float32\n",
      "Value: [[-0.17805344  0.6164113 ]]\n",
      "----------------------------------------\n",
      "Name: Dense_1/bias\n",
      "Shape: (1,)\n",
      "dtype: float32\n",
      "Value: [0.00985202]\n",
      "----------------------------------------\n",
      "Name: Dense_1/kernel\n",
      "Shape: (2, 1)\n",
      "dtype: float32\n",
      "Value: [[ 0.330739  ]\n",
      " [-0.07373184]]\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_params(state.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lorax.constants import LORA_FULL, LORA_FREEZE\n",
    "\n",
    "def decision_fn(path, param):\n",
    "    if 'embedding' in path:\n",
    "        print(f'Fully finetuning param {path}')\n",
    "        return LORA_FULL\n",
    "    dim = 3\n",
    "    print(f'Using LoRA with dim={dim} for param {path}')\n",
    "    return dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using LoRA with dim=3 for param (DictKey(key='Dense_0'), DictKey(key='kernel'))\n",
      "Using LoRA with dim=3 for param (DictKey(key='Dense_1'), DictKey(key='kernel'))\n"
     ]
    }
   ],
   "source": [
    "lora_spec = lorax.simple_spec(params, decision_fn=decision_fn, tune_vectors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Dense_0': {'bias': -1, 'kernel': 3}, 'Dense_1': {'bias': -1, 'kernel': 3}}"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_params = lorax.init_lora(params, lora_spec, jax.random.PRNGKey(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dense_0/bias\n",
      "Shape: (2,)\n",
      "dtype: float32\n",
      "Value: [0. 0.]\n",
      "----------------------------------------\n",
      "Name: Dense_0/kernel\n",
      "Shape: (1, 2)\n",
      "dtype: float32\n",
      "Value: LoraWeight(shape=(1, 2), dtype=dtype('float32'), w=Array([[-0.17805344,  0.61719763]], dtype=float32), a=Array([[ 0.0021586 , -0.00557593],\n",
      "       [ 0.00328551,  0.00336264],\n",
      "       [ 0.02129472, -0.000558  ]], dtype=float32), b=Array([[0., 0., 0.]], dtype=float32), alpha=1.0)\n",
      "----------------------------------------\n",
      "Name: Dense_1/bias\n",
      "Shape: (1,)\n",
      "dtype: float32\n",
      "Value: [0.]\n",
      "----------------------------------------\n",
      "Name: Dense_1/kernel\n",
      "Shape: (2, 1)\n",
      "dtype: float32\n",
      "Value: LoraWeight(shape=(2, 1), dtype=dtype('float32'), w=Array([[ 0.330739  ],\n",
      "       [-0.07981248]], dtype=float32), a=Array([[ 0.00025438],\n",
      "       [-0.00417447],\n",
      "       [-0.01784344]], dtype=float32), b=Array([[0., 0., 0.],\n",
      "       [0., 0., 0.]], dtype=float32), alpha=1.0)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_params(lora_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_model = lorax.lora(model.apply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array([[-0.04926008]], dtype=float32)"
      ]
     },
     "execution_count": 175,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lora_model({\"params\": lora_params}, jnp.ones(shape=(1, input_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "state = train_state.TrainState.create(\n",
    "    apply_fn=lora_model,  # forward pass func\n",
    "    params=lora_params,   # model weights\n",
    "    tx=lorax.wrap_optimizer(optax.sgd(learning_rate=0.1), lora_spec)  # optimizer func\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pass(params, state, batch):\n",
    "  input_images, labels = batch\n",
    "    \n",
    "  # call forward pass function.\n",
    "  logits = state.apply_fn({\"params\": params}, input_images)\n",
    "\n",
    "  # compute loss\n",
    "  loss = optax.squared_error(logits, labels)\n",
    "  loss = loss.mean()\n",
    "  return loss, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward_pass(state, batch):\n",
    "  # create a function to compute gradients wrt to loss\n",
    "  # returned by our `forward_pass` function.\n",
    "  grad_fn = jax.value_and_grad(forward_pass, argnums=(0), has_aux=True)\n",
    "\n",
    "  # compute gradients.\n",
    "  (loss, _), grads = grad_fn(state.params, state, batch)\n",
    "  print(grads)\n",
    "\n",
    "  # apply gradients.\n",
    "  state = state.apply_gradients(grads=grads)\n",
    "\n",
    "  return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(state, batch):\n",
    "  return backward_pass(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = (jnp.ones(shape=(1, input_dim)), jnp.zeros(shape=(1, output_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "for _ in range(1000):\n",
    "    state, loss = train_step(state, batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dense_0/bias\n",
      "Shape: (2,)\n",
      "dtype: float32\n",
      "Value: [ 0.        -0.0039066]\n",
      "----------------------------------------\n",
      "Name: Dense_0/kernel\n",
      "Shape: (1, 2)\n",
      "dtype: float32\n",
      "Value: LoraWeight(shape=(1, 2), dtype=dtype('float32'), w=Array([[-0.17805344,  0.61719763]], dtype=float32), a=Array([[ 0.0021586 , -0.00557593],\n",
      "       [ 0.00328551,  0.00336265],\n",
      "       [ 0.02129472, -0.000558  ]], dtype=float32), b=Array([[ 7.2609791e-06, -4.3788386e-06,  7.2662425e-07]], dtype=float32), alpha=1.0)\n",
      "----------------------------------------\n",
      "Name: Dense_1/bias\n",
      "Shape: (1,)\n",
      "dtype: float32\n",
      "Value: [0.04894758]\n",
      "----------------------------------------\n",
      "Name: Dense_1/kernel\n",
      "Shape: (2, 1)\n",
      "dtype: float32\n",
      "Value: LoraWeight(shape=(2, 1), dtype=dtype('float32'), w=Array([[ 0.330739  ],\n",
      "       [-0.07981248]], dtype=float32), a=Array([[ 0.00025439],\n",
      "       [-0.00417465],\n",
      "       [-0.01784424]], dtype=float32), b=Array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [ 2.5544762e-06, -4.1919662e-05, -1.7918246e-04]], dtype=float32), alpha=1.0)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_params(state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: Dense_0/bias\n",
      "Shape: (2,)\n",
      "dtype: float32\n",
      "Value: [ 0.         -0.00141436]\n",
      "----------------------------------------\n",
      "Name: Dense_0/kernel\n",
      "Shape: (1, 2)\n",
      "dtype: float32\n",
      "Value: LoraWeight(shape=(1, 2), dtype=dtype('float32'), w=Array([[-0.17805344,  0.61719763]], dtype=float32), a=Array([[-0.01458195, -0.0204706 ],\n",
      "       [-0.01424288,  0.011684  ],\n",
      "       [-0.00975838, -0.01271841]], dtype=float32), b=Array([[ 9.6509393e-06, -5.5084647e-06,  5.9961412e-06]], dtype=float32), alpha=1.0)\n",
      "----------------------------------------\n",
      "Name: Dense_1/bias\n",
      "Shape: (1,)\n",
      "dtype: float32\n",
      "Value: [0.01772106]\n",
      "----------------------------------------\n",
      "Name: Dense_1/kernel\n",
      "Shape: (2, 1)\n",
      "dtype: float32\n",
      "Value: LoraWeight(shape=(2, 1), dtype=dtype('float32'), w=Array([[ 0.330739  ],\n",
      "       [-0.07981248]], dtype=float32), a=Array([[-0.00066073],\n",
      "       [ 0.00166766],\n",
      "       [ 0.01177997]], dtype=float32), b=Array([[ 0.0000000e+00,  0.0000000e+00,  0.0000000e+00],\n",
      "       [-2.4075107e-06,  6.0765069e-06,  4.2922969e-05]], dtype=float32), alpha=1.0)\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print_params(state.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "INVALID_ARGUMENT: Invalid buffer passed to Execute() as argument 0 to replica 0: INVALID_ARGUMENT: Donation requested for invalid buffer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[138], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m merged_params \u001b[38;5;241m=\u001b[39m \u001b[43mlorax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlora_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/lorax/helpers.py:94\u001b[0m, in \u001b[0;36mmerge_params\u001b[0;34m(lora_params, destructive, use_scaling)\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m result\n\u001b[1;32m     92\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m param\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mqax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mutils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_map_with_implicit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmap_fn\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlora_params\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/qax/implicit/implicit_utils.py:35\u001b[0m, in \u001b[0;36mcombine_leaf_predicate.<locals>.new_fn\u001b[0;34m(new_is_leaf, *args)\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombined_is_leaf\u001b[39m(arg):\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m is_leaf(arg) \u001b[38;5;129;01mor\u001b[39;00m new_is_leaf(arg)\n\u001b[0;32m---> 35\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbase_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_leaf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcombined_is_leaf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/jax/_src/tree_util.py:343\u001b[0m, in \u001b[0;36mtree_map\u001b[0;34m(f, tree, is_leaf, *rest)\u001b[0m\n\u001b[1;32m    341\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    342\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtreedef\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mxs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mall_leaves\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/jax/_src/tree_util.py:343\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    341\u001b[0m leaves, treedef \u001b[38;5;241m=\u001b[39m tree_flatten(tree, is_leaf)\n\u001b[1;32m    342\u001b[0m all_leaves \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treedef\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rest]\n\u001b[0;32m--> 343\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treedef\u001b[38;5;241m.\u001b[39munflatten(\u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mxs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m xs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mall_leaves))\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/lorax/helpers.py:88\u001b[0m, in \u001b[0;36mmerge_params.<locals>.map_fn\u001b[0;34m(param)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmap_fn\u001b[39m(param):\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param, LoraWeight):\n\u001b[0;32m---> 88\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mmaterializer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m destructive:\n\u001b[1;32m     90\u001b[0m             jax\u001b[38;5;241m.\u001b[39mtree_map(_ensure_delete, param)\n",
      "\u001b[0;31mValueError\u001b[0m: INVALID_ARGUMENT: Invalid buffer passed to Execute() as argument 0 to replica 0: INVALID_ARGUMENT: Donation requested for invalid buffer"
     ]
    }
   ],
   "source": [
    "merged_params = lorax.merge_params(lora_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_model_output = model.apply({\"params\": merged_params}, jnp.ones(shape=(1, input_dim)))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "private_outputs": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
